# NIPS2020_DRL

1.《Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model》
- 关键词：model-based reinforcement learning, minimaxity, planning, policy evaluation, instance-dependent guarantees, generative model
- 从理论上研究了样本复杂度和统计准确性之间的权衡取舍。得出了基于模型的策略评估的改进（instance-dependent）保证，据我们所知，该工作提供了生成模型中第一个极大极小最优保证，可容纳the entire range of sample sizes.

2.《Deep Reinforcement and InfoMax Learning》
- 关键词： predictive of the future，InfoMax Learning，representations
- 我们的工作基于以下假设：representations可以预测未来状态的属性的无模型的智能体，将更有能力解决和适应新的RL问题。为了检验这一假设，我们引入了一个基于Deep InfoMax（DIM）的目标，该目标通过最大化其内部表示的连续时间步长间的互信息来训练智能体预测未来。我们从马尔科夫链混合时间的角度对方法的收敛特性进行了直观分析，并认为互信息下限的收敛性与过渡模型的逆绝对谱隙有关。我们在几个合成环境中测试了新方法，它成功地学习了对未来有预测性的表示。最后，我们用temporal DIM目标增强了C51，一个强大的RL基线，并在持续学习任务和最近引入的Procgen环境上展示了改进的性能。

3.《Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition》
- 关键词：Model-Free RL
- 我们研究了在具有S个状态、A个动作和episode 长度为H的finite-horizon episodic马尔科夫决策过程（MDPs）环境下的强化学习问题，提出了一种有不错理论保证的无模型算法UCB-Advantage。UCB-Advantage实现了较低的局部切换成本，并适用于并发强化学习，它在[Bai等，2019]的最新结果基础上进行了改进。

4.《Effective Diversity in Population Based Reinforcement Learning》
- 关键词：Population、exploration、diversity
- 探索是强化学习中的一个关键问题，因为智能体只能从他们在环境中获得的数据中学习。考虑到这一点，维持一个智能体群体是一种有吸引力的方法，因为它允许收集具有多样化行为的数据。这种行为多样性通常通过多目标损失函数来鼓励。然而，这些方法通常利用基于对偶距离的平均场更新，这使它们很容易受到循环行为和增加冗余的影响。此外，明确鼓励多样性往往对优化已有成果的行为进行奖励有不利影响。因此，奖励-多样性的权衡通常依赖于启发式方法。最后，这类方法需要的行为表示通常是手工制作的和特定领域的。在本文中，我们介绍了一种同时优化一个种群所有成员的方法。我们没有使用对偶距离，而是测量整个种群在behavioral manifold中的体积，这由任务无关的行为（behavioral）嵌入定义。此外，新算法Diversity via Determinants (DvD)在训练过程中使用在线学习技术调整多样性程度。我们介绍了DvD的进化和基于梯度的实例，并表明当不需要更好的探索时，它们可以有效改善探索而不降低性能。

5.《A Boolean Task Algebra for Reinforcement Learning》
- 关键词：Boolean Task Algebra、multi-task
- 我们提出了一个在任务空间上定义布尔代数的框架。这使得我们可以用一组基础任务的否定、disjunction和连接来制定新任务。文章表明，通过学习面向目标的价值函数和限制任务的过渡动态，智能体可以在不进一步学习的情况下解决这些新任务。我们证明，通过以特定的方式组合这些价值函数，我们立即恢复了布尔代数下可表达的所有任务的最优策略。我们在两个领域（包括一个需要函数逼近的高维视频游戏环境）验证了新方法，实验中智能体首先学习一组基本技能，然后将它们组合起来，解决超指数数量的新任务。

6.《Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control》
- 没找到paper

7.《Multi-task Batch Reinforcement Learning with Metric Learning》
- 关键词：Multi-task，Batch RL
- 我们解决了多任务Batch RL问题。给定从不同任务中收集的多个数据集，我们训练一个多任务策略，使其在从相同分布中采样的未见任务中表现良好。为了表现良好，策略必须通过建模其对状态、动作和奖励的依赖性，从收集到的transitions中推断出任务身份。由于不同数据集可能具有差异较大的状态-动作分布，任务推理模块可能会学习忽略奖励，只将状态-动作对虚假地与任务身份相关联，从而导致测试时间性能不佳。为了鲁棒化任务推理，我们提出了一种新型的triplet loss的应用。为了挖掘hard negative examples，我们通过近似训练任务的奖励函数**来重新标记训练任务的transitions**。当我们允许在未见任务上进行进一步的训练时，使用之前训练了的策略作为初始化，与随机初始化的策略相比，收敛速度显著加快（高达80%的改进，并且跨越5种不同的Mujoco任务分布）。我们将新方法命名为MBML（Multi-task Batch RL with Metric Learning）。

7.《On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems》
- 没找到文章

8.《Towards Playing Full MOBA Games with Deep Reinforcement Learning》
- 没找到文章，不过有其他相关文章的解读 https://zhuanlan.zhihu.com/p/99210924

9.《Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting》
- 关键词：FMDPs、Non-Episodic
- 我们研究non-episodic factored马尔科夫决策过程（FMDPs）中的强化学习。我们 1.提出了两种近乎最优的、oracle-efficient 的FMDPs算法；2.为FMDPs提出了一个更严格的连通性度量——factored span，并证明了一个取决于factored span而不是直径D的下界。为减小下界和上界之间的差距，我们提出了对REGAL.C算法的改编，其后悔界取决于factored span。我们的oracle-efficient算法在计算机网络管理模拟上优于之前提出的接近最优的算法。

10.《Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning》
- 关键词：MARL、Policy regularization、Coordination
- 在MARL中，发现成功的集体行为是具有挑战性的，因为它需要探索一个联合行动空间，这个空间随着智能体数量的增加而呈指数增长。虽然独立智能体探索的可操作性很吸引人，但这种方法在需要详细群体策略的任务上却失败了。我们认为，协调智能体的策略可以指导探索，我们研究了促进这种归纳偏置的技术，提出了两种策略正则化方法——基于智能体间行动可预测性的TeamReg，以及依赖于同步行为选择的CoachReg。我们在四个具有挑战性的连续控制任务上对每种方法进行评估，这些任务具有稀疏奖励，且需要不同程度的协调。实验中我们发现，相比于其他baselines，新方法对超参数的变化更加稳健。同时，新方法可成功协调不同智能体的行为，显著提高了合作性多智能体问题的性能，并且当智能体数量增加时，新方法可以很好地扩展。

11.《Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning》
- 关键词：Off-policy、Confounding-Robust、olicy Evaluation
- batch rl中从观测数据中对顺序决策策略进行Off-policy评估是必要的。然而，未观察到的变量会混淆观察到的行动，使得新策略的精确评估不可能。为此，我们开发了一种稳健的方法，在给定数据的情况下，通过敏感度模型，在infinite-horizon问题中估计给定策略的（不可识别的）价值的尖锐边界，这些数据来自另一个具有未观测的 confounding。我们考虑了静止的或 baseline unobserved confounding，并通过优化所有与新的部分识别估计方程和敏感性模型一致的静止状态占用率的集合来计算边界。当我们收集更多的confounding数据时，我们证明了对 sharp bounds的收敛性。虽然检查set membership是一个线性规划，但support函数是由一个困难的非凸优化问题给出的。我们开发了基于非凸投射梯度下降的近似方法，并以经验证明了结果的边界。

12.《Learning Retrospective Knowledge with Reverse Reinforcement Learning》
- 关键词：Retrospective Knowledge、Reverse RL
- 我们提出了一种逆强化学习（Reverse RL）的方法来表示Retrospective Knowledge。一般的价值函数（GVF）在表示预测性知识方面（即回答关于未来可能结果的问题，如 “如果我们开车从A到B，预计会消耗多少燃料？”）取得了巨大成功。然而，GVFs无法回答 “如果一辆汽车在时间t时在B处，我们期望它耗费多少燃料？”这样的问题。要回答此问题，我们需要知道那辆车什么时候加满了油，以及是如何到达B的，由于这类问题强调的是过去可能发生的事件对现在的影响，我们将其答案称为Retrospective Knowledge。在本文中，我们展示了如何用Reverse GVF来表示回顾性知识，它是通过Reverse RL来训练的。我们用经验证明了逆GVFs在表征学习和异常检测中的效用。

13.《Combining Deep Reinforcement Learning and Search for Imperfect-Information Games》
- 关键词：Games theory、Imperfect-Information
- 在训练和测试时进行DRL和搜索的结合是一个强大的paradigm，它导致了单智能体设置和完美信息游戏的许多成功案例，其中最成功就是AlphaZero。 但是，这种形式的算法无法应付不完美的信息游戏。 本文介绍了ReBeL，这是一个用于self-play RL和搜索不完全信息游戏的通用框架。 在更简单的完美信息游戏环境中，ReBeL简化为类似于AlphaZero的算法。 结果表明，ReBeL导致基准不完全信息游戏中的可利用性较低，并在heads-up no-limit德州扑克中获得超人表现，同时使用的领域知识比以前的任何扑克AI都要少。 我们还证明了ReBeL在 tabular settings的两人零和游戏中收敛到Nash平衡。

14.《Reinforced Molecular Optimization with Neighborhood-Controlled Grammars》
- 没找到文章

15.《POMO: Policy Optimization with Multiple Optima for Reinforcement Learning》
- 没找到文章

16.《Self-Paced Deep Reinforcement Learning》
- 关键词：Curriculum Reinforcement Learning (CRL) 、reasoning、automatic curriculum generation
- 课程强化学习(Curriculum Reinforcement Learning，CRL)通过在整个学习过程中让智能体接触到一系列量身定制的任务，提高智能体的学习速度和稳定性。尽管在经验上取得了成功，但CRL中的一个未决问题是如何为给定的强化学习（RL）智能体自动生成课程以避免人工设计。在本文中，我们提出了一个答案，将课程生成解释为一个推理问题，其中任务上的分布被逐步学习以接近目标任务。这种方法导致了一种自动的课程生成，它的pace由智能体控制，控制过程具有坚实的理论动机，并且很容易与DRL算法耦合。在实验中，新算法生成的课程显著提高了在几种环境和DRL算法中的学习性能，与最先进的CRL算法相匹配或优于后者。

17.《Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning》
- 关键词：exploration、Model-Based RL
- 基于模型的强化学习算法和概率动力学模型是数据效率最高的学习方法之一。 这通常归因于他们区分认知和不确定不确定性的能力。但是，虽然大多数算法在学习模型时都将这两个不确定性区分开来，但在优化策略时却忽略了它。 在本文中，我们证明了忽略认知不确定性会导致贪婪算法无法充分探索。 反过来，我们提出了一种实用的乐观探索算法（H-UCRL），该算法利用幻觉输入（hallucinated inputs）扩大了输入空间，该幻觉输入可施加模型中认知不确定性所能提供的尽可能多的控制。 我们分析了这种情况，并为校准良好的模型构建了一个general regret bound。 基于这一理论基础，我们展示了如何将乐观探索与最新的强化学习算法和不同的概率模型轻松地结合在一起。我们的实验表明，当存在对行动的惩罚时（这对于其他现有的基于模型的强化学习算法来说十分困难），乐观探索显著加快了学习速度。

18.《Weakly-Supervised Reinforcement Learning for Controllable Behavior》
- 关键词：Weakly-Supervised
- Q：我们是否可以将任务空间限制为语义上有意义的任务？
- A：在这项工作中，我们介绍了一个框架，该框架使用弱监督**自动将任务的语义有意义的子空间与无意义的“chaff”任务的巨大空间自动区分开。 我们表明，该学习的子空间能够进行有效探索，并提供捕获状态之间距离的表示**。 在各种具有挑战性的，基于视觉的连续控制问题上，我们的方法可带来可观的性能提升，尤其当环境复杂性不断提高时。

19.《MOReL: Model-Based Offline Reinforcement Learning》
- 关键词：Model-Based RL、offline RL
- 在offline RL中，目标是仅基于与环境发生历史交互的数据集学习高回报的策略。 离线训练RL策略的能力可以大大扩展RL的适用性，数据效率和实验速度。 offline RL中的先前工作几乎仅限于无模型RL方法。 在这项工作中，我们提出MOReL，这是用于基于模型的offline RL的算法框架。 该框架包括两个步骤：（a）使用离线数据集学习悲观的MDP（P-MDP）； （b）在该P-MDP中学习接近最优的策略。 获知的P-MDP具有以下特性：对于任何策略，实际环境中的性能大约都受到P-MDP中性能的限制。 这使其可以作为策略评估和学习目的的良好替代，并且可以克服基于模型的RL（如model exploitation）的常见陷阱。 从理论上讲，我们显示MOReL对于offline RL是几乎minimax最优的。 通过实验，我们显示MOReL在经过广泛研究的离线RL基准测试中达到或超过了最新结果。 此外，MOreL的模块化设计使其相关组件的未来发展（如，生成建模，不确定性估计，规划等）可直接转化为offline RL的发展。

20.《Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension》
- 关键词：Function Approximation
- 值函数逼近已证明在强化学习（RL）中取得了惊人的经验成功。 然而，尽管最近在发展具有线性函数逼近的RL理论上取得了一些进展，但对通用函数逼近方案的理解仍然很不足。 在本文中，我们建立了一种通用值函数近似的可证明有效的RL算法。我们的理论使用线性值函数逼近来概括RL的最新进展，且新算法是无模型的，我们也没对环境做出明确假设。

21.《Security Analysis of Safe and Seldonian Reinforcement Learning Algorithms》
- 没找到文章

22.《Model-based Adversarial Meta-Reinforcement Learning》
- 关键词：meta-RL、Adversarial、gradient estimator
- 元强化学习（meta-RL）旨在从多个训练任务中学习有效地适应未曾见过的测试任务的能力。 尽管取得了成功，但已知现有的meta-RL算法对任务分配转移很敏感。 当测试任务分配与训练任务分配不同时，性能可能会大大降低。 为了解决这个问题，本文提出了基于模型的对抗性元强化学习（AdMRL），我们旨在最大程度地减少最坏情况的次优gap-最优回报与算法适应后获得的回报间的gap -使用基于模型的方法来处理一系列任务中的所有任务。 我们提出了一个minimax目标，并通过在固定任务上学习动力学模型与在当前模型的对抗任务之间进行交替来优化它-该任务所导致的策略在最大程度上次优。 假设任务族已参数化，我们通过隐函数定理推导次优梯度相对于任务参数的公式，并说明如何通过共轭梯度法和新颖的方法有效地实现梯度估计器 REINFORCE估算器。 我们在几个连续的控制基准上评估了新方法，并证明了它在所有任务的最坏情况下的性能，对 out-of-distribution任务的泛化能力以及在现有状态下的训练和测试时段样本效率方面的功效。

23.《Safe Reinforcement Learning via Curriculum Induction》
- 关键词：Curriculum learning、safe RL
- 在对安全性要求严格的应用中， autonomous agents可能需要在错误可能造成巨大损失的环境中学习。 在这种情况下，智能体要在学习之后和学习过程中安全行事。 为达到此目的，现有的安全强化学习方法使智能体依赖先验条件，从而有可能避免探索过程中的危险情况，但是先验条件固有的概率保证和平滑假设在许多场景如 自动驾驶中均不可行。 本文提出了一种受人类教学启发的替代方法，其中智能体在自动指导员的指导下进行学习，从而避免了在学习过程中违反约束。 在此模型中，我们引入的监视器既不需要知道智能体在学习的任务上如何做好，也不需要知道环境如何工作。 相反，它具有重置控制器库，当智能体开始出现危险行为时可激活重置控制器，以防止智能体造成损坏。 至关重要的是，在哪种情况下使用哪种重置控制器会影响智能体学习的速度。 基于观察智能体的进度，老师自己会学习选择重置控制器的策略和课程表，以优化智能体的最终策略奖励。 我们的实验在两个环境中使用此框架来诱导课程的安全有效学习。

24.《Conservative Q-Learning for Offline Reinforcement Learning》
- 关键词：Conservative Q-Learning 、regularization
- 有效地利用强化学习（RL）中以前收集的大型数据集是大规模实际应用的主要挑战。 离线RL算法保证无需进一步交互即可从以前收集的静态数据集中学习有效的策略。 但是，在实践中，离线RL提出了一个重大挑战，标准的off-policy RL方法可能会因对数据集和学习的策略之间的分布偏移而导致的值进行过高估计而失败，尤其是在对复杂和多模态数据分布进行训练时 。 在本文中，我们提出了保守的Q学习（CQL），其目的是通过学习保守的Q函数来解决这些限制，从而使该Q函数下策略的期望值lower-bounds其真实值。 我们从理论上证明CQL对当前策略的价值产生了下界，并且可以将其纳入具有理论改进保证的策略学习过程中。 在实践中，CQL通过简单的Q值正则化器扩展了标准的Bellman错误目标，该Q值正则化器可在现有的DQN和基于actor的实施上直接实现。 在离散和连续控制域上，我们都表明CQL大大优于现有的离线RL方法，经常学习的策略可以获得更高的2-5倍的最终回报（尤其是从复杂的多模态数据分布中学习时）。

25.《Munchausen Reinforcement Learning》
- 关键词：current policy、scaled log-policy
- Bootstrapping 是强化学习（RL）中的核心机制。 大多数算法基于temporal differences，以其对当前值的估计来代替过渡状态的真实值。 但是，我们还可以利用current policy估计来引导RL。 我们的核心贡献在于一个非常简单的想法：将scaled log-policy添加到即时奖励中。 我们证明，以这种方式稍加修改Deep Q-Network（DQN）即可提供一种与Atari游戏上的分配方法有竞争力的智能体，而无需利用distributional RL, n-step returns or prioritized replay。 为证明这种想法的多功能性，我们还将其与隐式分位数网络（IQN）结合使用。 为继续给这项经验研究添色，我们提供了关于幕后发生的强大理论见解-隐式Kullback-Leibler正则化和action-gap的增加。

26.《Non-Crossing Quantile Regression for Distributional Reinforcement Learning》
- 没找到文章

27.《Online Decision Based Visual Tracking via Reinforcement Learning》
- 没找到文章

28.《Discovering Reinforcement Learning Algorithms》
- 关键词：meta learning
- 强化学习(RL)算法根据多年研究中人工发现的几种可能的规则之一更新智能体的参数。从数据中自动发现更新规则可以带来更高效的算法，或更好地适应特定环境的算法。虽然之前已经有人尝试解决这一挑战，但发现RL的基本概念（如值函数和时差学习）的替代方案是否可行仍是一个悬而未决的问题。本文引入了一种新的元学习方法，通过与一组环境的交互，发现整个更新规则，其中包括 “预测什么”（如价值函数）和 “如何从中学习”（如bootstrapping）。新算法的输出是一个RL算法，我们称之为学习策略梯度（LPG）。实证结果表明，我们的方法发现了自己对价值函数概念的替代。此外，它还发现了一种bootstrapping机制来维持和使用其预测。令人惊讶的是，当仅在玩具环境中进行训练时，LPG有效地泛化到复杂的Atari游戏中，并取得了非平凡的性能。这表明了从数据中发现一般RL算法的潜力。

29.《Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning》
- 关键词：MARL、Shared Experience
- 在MARL中的探索是一个具有挑战性的问题，尤其在奖励稀少的环境中。 我们建议通过在智能体之间共享经验来进行有效探索的通用方法。 我们提出的算法称为“Shared Experience Actor-Critic”（SEAC），将经验分享应用于actor-Critic框架。 我们在稀疏奖励多智能体环境的集合中评估了SEAC，发现它以更少的步骤学习并收敛到更高的回报，始终优于两个基准和两个最新算法。 在某些更艰难的环境中，经验共享会在学习解决任务和根本不学习之间体现出性能差别。

30.《The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning》
- 关键词：evaluation
- 我们研究了评估RL方法基于模型的行为的度量标准——Local Change Adaptation（LoCA），它可以衡量RL方法适应环境中Local Change的速度。

31.《Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning》
- 关键词：Regularization、ADP
- 我们研究了Kullback-Leibler（KL）和熵正则化在强化学习中的作用。 通过相关近似动态规划（ADP）方案的等效表示，我们表明KL惩罚等于平均q值。 这种等价性可以在文献中的先验不相干的方法之间建立联系，并证明KL正则化确实会导致在每次迭代值函数更新时做出的平均误差。 通过理论分析，我们还研究了KL和熵正则化之间的相互作用。 当考虑的ADP方案与基于神经网络的随机逼近相结合时，等价性就会丢失，这表明了进行正则化的许多不同方法。

32.《Task-agnostic Exploration in Reinforcement Learning》
- 关键词：exploration、Task-agnostic、multi-task
- 有效的探索是强化学习（RL）的主要挑战之一。 大多数现有的采样有效算法都假设在探索过程中存在单个奖励函数。 但是，在许多实际情况下，例如，当一个智能体需要同时学习许多技能，或者需要平衡多个相互矛盾的目标时，就没有单一的基础奖励函数来指导探索。 为了解决这些挑战，我们提出了task-agnostic RL框架：在探索阶段，智能体首先通过探索MDP来收集轨迹，而无需奖励函数的指导。 经过探索，它的目的是为N个任务找到接近最佳的策略, given the collected trajectories augmented with sampled rewards for each task。 我们提出了一种高效的与任务无关的RL算法UCBZero，UCBZero的理论性能十分不错。

33.《Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning》
- 关键词：HRL、Adjacency-Constrained Subgoals、search
- Goal-conditioned分层强化学习（HRL）是扩大强化学习（RL）技术的一种有前途的方法。 但是，由于大的目标空间，此算法训练效率低下。 在较大的目标空间中进行搜索会给高级子目标生成和低级策略学习带来困难。 在本文中，我们表明可以通过使用邻接约束将高级动作空间从整个目标空间限制到以当前状态为中心的k步邻接区域来有效缓解此问题。 我们从理论上证明了邻接约束保留了最佳的分层策略，并表明该约束可通过训练可以区分相邻和不相邻子目标的邻接网络来实际实现。 在离散和连续控制任务上的实验结果表明，我们的方法优于最新的HRL方法。

34.《Reinforcement Learning with Feedback Graphs》
- 关键词：episodic RL、Feedback Graphs、model-based RL
- 我们研究马尔科夫决策过程中的episodic RL，此时智能体每一步都会收到几个transition observations形式的额外反馈。通过扩展的传感器或关于环境的先验知识（例如，当某些动作产生类似结果时），在一系列任务中可获得这样的额外观察。我们使用状态-动作对的反馈图来形式化这种设置，并表明基于模型的算法可利用额外的反馈来进行更有效的样本学习。我们给出了一个忽略对数因素和低阶项的遗憾边界，该边界仅取决于反馈图的最大无环子图的大小，而在没有反馈图的情况下，该边界对状态和动作的数量具有多项式依赖性。最后，我们强调了与bandit环境相比利用反馈图的小支配集时的挑战，并提出了一种新的算法，该算法可以利用这种支配集的知识以更sample-efficient地学习近似最优策略。

35.《Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning》
- 关键词：Storage、Runtime Channel Pruning
- 在本文中，我们提出了一种基于DRL的框架，以在CNN上有效执行runtime channel pruning。我们基于DRL的框架旨在学习一种修剪策略，以确定在每个卷积层中要修剪多少通道以及哪些通道（depending on each specific input instance in runtime）。新策略通过在总体计算预算下限制不同层上的计算资源以优化网络性能。此外，与其他需要在推理中存储所有通道参数的其他runtime channel方法不同，我们的框架可以通过引入静态修剪组件来减少部署时的参数存储消耗。

36.《Multi-Task Reinforcement Learning with Soft Modularization》
- 关键词：Multi-Task、 Soft Modularization
- 主要亮点：
通过定义可微分的总加权目标函数，将路由网络（用于控制对子网络各层赋予的不同权重）的训练和
各子网络的训练协同，而不是单独用 RL 再训练路由网络；
将总目标函数中对不同子目标赋予的权重巧妙地和与熵有关的参数 α 相关联。因为不同子网络的熵
能反映它们不同的训练程度，所以新算法对解决 MTLRL 中的分心困境（根据不同子任务的状态，合理
平衡对它们赋予的不同注意力）有一定帮助。

37.《Weighted QMIX: Improving Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning》
- 关键词：MTRL、centralised
- 在许多实际环境中，一组智能体必须在以分散方式行事的同时协调其行为。同时，通常可以用集中式的方式训练智能体，在这种情况下，全局状态信息是可用的，并且通信约束被解除。学习以额外状态信息为条件的联合行动值是利用集中式学习的一种有吸引力的方式，但随后提取分散式策略的最佳策略还不清楚。我们的解决方案是QMIX，这是一种新颖的基于价值的方法，它可以以集中式的端到端方式训练分散式策略。QMIX采用了一个混合网络，将联合行动值估计为每个智能体值的单调组合。我们在结构上强制要求联合行动值在每个智能体值中是单调的，通过使用混合网络中的非负权重，保证了集中式和分散式策略之间的一致性。为了评估QMIX的性能，我们提出星际争霸多智能体挑战赛（SMAC）作为深度多智能体强化学习的新基准。我们在一组具有挑战性的SMAC场景上对QMIX进行了评估，并表明它的性能显著优于现有的多智能体强化学习方法。

38.《MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning》
- 关键词：Homomorphic Networks、constraint
- 本文介绍了用于深度强化学习的MDP同态网络。 MDP同态网络是在MDP的联合状态-动作空间中的对称性下等价的神经网络。 通过使用等方差约束将此先验知识构建到策略和价值网络中，我们可以减小解空间的size。 我们特别关注组结构对称（可逆转换）。另外，我们引入了一种简单方法来数值构造等变网络层，因此系统设计人员无需像通常那样手动解决约束。 我们构造了在一组反射或旋转下等变的MDP同态MLP和CNN。 我们证明，在CartPole，网格世界和Pong上，此类网络的收敛速度比非结构化baseline更快。

39.《On Efficiency in Hierarchical Reinforcement Learning》
- 关键词：HRL、Efficiency
- 未找到文章

40.《Variational Policy Gradient Method for Reinforcement Learning with General Utilities》
- 关键词：Variational Policy Gradient、Utilities
- 本文考虑了马尔可夫决策问题中的策略优化，其中目标是state-action occupancy measure的一般凹效用函数。 这样的普遍性使Bellman方程无效。 由于这意味着动态规划不再起作用，因此我们专注于直接的策略搜索。 类似于可用于带有累积奖励的RL策略梯度定理\ cite {sutton2000policy}，我们导出了具有通用效用的新RL变分策略梯度定理，它确定了可以通过参数化的策略梯度作为随机鞍点的解（涉及效用函数的Fenchel对偶问题）。 我们开发了一种变分的蒙特卡洛梯度估计算法，以基于样本路径计算策略梯度，并且证明，尽管优化问题是非凸的，但变分策略梯度方案在全局上收敛到了针对一般目标的最优策略。 我们还通过利用问题的隐藏凸度来建立阶数O（1 / t）的收敛速度，并证明当问题允许隐藏强凸度时，它的收敛速度为指数级。 我们的分析也适用于具有累积奖励的标准RL问题（特例）并可提高其收敛速度。

41.《Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs》
- 关键词：Model-based RL、SMDPs、Neural ODEs

42.《Reinforcement Learning with Augmented Data》
- 关键词：data Augmentation
- 从视觉观察中学习是强化学习(RL)中一个基本而又具有挑战性的问题。尽管算法的进步与卷积神经网络的结合已被证明是成功的秘诀，但目前的方法在以下两方面仍有欠缺。(a)学习的数据效率和(b)对新环境的泛化。为此，我们提出了增强数据的强化学习（RAD），这是一个简单的即插即用模块，可以增强大多数RL算法。我们首次对基于像素和基于状态的输入的RL的通用数据增强进行了广泛的研究，并引入了两种新的数据增强–随机翻译和随机振幅尺度。我们表明，随机转换、裁剪、颜色抖动、补丁切除、随机卷积和振幅尺度等增强功能可以使简单的RL算法在通用基准上优于复杂的最先进方法。RAD在数据效率和基于像素控制的DeepMind Control Suite基准以及基于状态控制的OpenAI Gym基准的最终性能方面创造了一个新sota。我们进一步证明，在几个OpenAI ProcGen基准上，RAD比现有方法显著改善了测试阶段的泛化。
- 代码：https://github.com/MishaLaskin/rad

43.《Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing》
- 关键词：combinatorial optimization
- 我们开发了一个具有组合动作空间的基于价值函数的深度强化学习框架，在该框架中，动作选择问题被明确地表述为混合整数优化问题。 作为一个激励性的例子，我们提出了该框架在capacitated vehicle routing problem（CVRP）中的应用。 在每种情况下，我们都将动作建模为单个车辆的整个行程，并考虑确定性策略，该策略可通过简单的策略迭代算法进行改进。 我们的方法可与其他强化学习方法竞争，并且在中等大小的标准库实例上产生接近最佳的结果。

44.《DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction》
- 关键词：data distribution
- 深度强化学习由于不稳定和对超参数的敏感性常常难以使用。当使用标准的监督方法（如，针对bandits）时，on-policy数据收集会提供“hard negatives”，它恰恰在策略可能访问的那些状态和行动中修正了模型。我们将这种现象称为 “矫正反馈”。我们表明，基于bootstrapping的Q-learning算法不一定能从这种纠正性反馈中获益，对算法收集的经验进行训练并不足以纠正Q函数的错误。事实上，Q-learning和相关方法可能会在智能体收集的经验分布和对该经验进行训练所诱导的策略之间表现出病态的相互作用，导致潜在的不稳定性、次优的收敛性，以及从嘈杂、稀疏或延迟的奖励中学习时的糟糕结果。我们从理论和经验上证明了这个问题的存在性。然后我们表明，对数据分布进行特定的修正可以缓解这个问题。基于这些观察，我们提出了一种新算法DisCor，它可以计算出最佳分布的近似值，并用它来重新加权用于训练的转换，从而在一系列具有挑战性的RL设置中获得实质性的改进，例如多任务学习和从嘈杂的奖励信号中学习。博客：https://bair.berkeley.edu/blog/2020/03/16/discor/

45.《Neurosymbolic Reinforcement Learning with Formally Verified Exploration》
- 关键词：safe RL、Neurosymbolic、mirror descent
- 我们提出了Revel——一种部分神经强化学习（RL）框架，用于在连续状态和动作空间中进行可证明的安全探索。 可证明安全的深度RL的关键挑战是，在 learning loop中反复验证神经网络在计算上是不可行的。 我们使用两个策略类来解决这个挑战：一个是具有近似梯度的一般神经符号类，另一个是允许高效验证的更限制的符号策略类。我们的学习算法是对策略的镜像下降：在每次迭代中，它都会安全地将一个符号策略提升到神经符号空间，对产生的策略进行安全的梯度更新，并将更新后的策略投射到安全的符号子集中，所有这些都不需要神经网络的明确验证。我们的实证结果表明，Revel在许多场景中强制执行安全探索，而约束策略优化则没有。

46.《Generalized Hindsight for Reinforcement Learning》
- 关键词：multi-task、Hindsight
- 强化学习（RL）中样本复杂性高的主要原因之一是无法将知识从一项任务转移到另一项任务。 在标准的多任务RL设置中，尝试解决一项任务时收集的低奖励数据几乎没有提供解决该特定任务的信号因此而被浪费。 但是，我们认为这些数据可能会为其他任务提供丰富的信息来源。 为了利用这种洞察力并有效地重用数据，我们提出了通用Hindsight：一种近似的逆强化学习技术，用于用正确的任务重新标记行为。 与标准的重新标记技术相比，Generalized Hindsight提供了更有效的样本重用，我们将在一组多任务导航和操纵任务上进行经验演示。 视频和代码：https://sites.google.com/view/generalized-hindsight

47.《Meta-Gradient Reinforcement Learning with an Objective Discovered Online》
- 关键词：Meta learning
- DRL的很多算法通过深度神经网络对内部表示（如价值函数或策略）进行参数化。每个算法都会根据一个目标（如Q-learning或策略梯度）来优化其参数。在这项工作中，我们提出了一种基于元梯度下降的算法，该算法仅从与环境的交互经验中发现目标，并由深度神经网络灵活地设定参数。随着时间的推移，智能体可以学习如何越来越有效地学习。此外，由于目标是被在线挖掘的，它可以随时间的推移而自适应变化。我们证明了该算法挖掘如何解决RL中的几个重要问题，如bootstrapping、非平稳性和off-policy学习。在Atari学习环境上，元梯度算法随着时间的推移适应了更高的学习效率，最终超越了强actor-critic基线的中位数得分。

48.《TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search》
- 关键词：search、curriculum learning
- 我们提出TorsionNet，这是一种在刚性转子近似下基于强化学习的有效顺序Conformer搜索技术。 该模型是通过课程学习训练的，课程学习将详细探讨其理论价值，以使基于热力学的新颖度量（Gibbs评分）最大化。

49.《Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning》
- 关键词：combinatorial optimization
- 未找到文章

50.《Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?》
- 关键词：Efficient
- 未找到文章

51.《Instance-based Generalization in Reinforcement Learning》
- 关键词：Instance-based、Generalization
- 传统强化学习（RL）算法在具有离散状态空间的域上运行。 它们通常表示表中的值函数，按状态或状态-动作对进行索引。 但是，将RL应用于具有连续状态的域时，表格表示形式不再可能。 在这些情况下，一种通用的方法是通过存储一小组状态（或状态-动作对）的值并将这些值插值到其他未存储的状态（或状态-动作对）来表示值函数。 这种方法称为基于实例的强化学习（IBRL）。 实例是显式存储的值，且插值通常使用众所周知的基于实例的监督学习算法。

52.《Preference-based Reinforcement Learning with Finite-Time Guarantees》
- 关键词：reward、Preference-based、 dueling bandits
- 基于偏好的强化学习（Preference-based Reinforcement Learning，PbRL）在传统的强化学习中用偏好来代替奖励值，以更好地引起人们对目标的意见，特别是在数值奖励难以设计或解释的情况下。尽管PbRL在应用中取得了可喜的成果，但对它的理论认识仍处于起步阶段。在本文中，我们首次提出了针对一般PbRL问题的Finite-Time分析。我们首先表明，如果对轨迹的偏好是确定性的，那么对于PbRL，唯一的最优策略可能不存在。如果偏好是随机的且偏好概率与隐藏的奖励值有关，那么无论有无模拟器，PbRL都能以高概率确定最佳策略。我们的方法通过导航到未被探索的状态来探索状态空间，并使用dueling bandits和策略搜索的组合来求解PbRL。

53.《Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes》
- 关键词：bandits、clustering、Q-learning
- 这项工作表明，强化学习可成功应用于解码短到中等长度的基于稀疏图的信道码。我们利用一种顺序更新策略，选择最佳的检查节点（CN）调度，以提高解码性能。特别地，我们将CN更新过程建模为一个多臂的、具有依赖臂的bandits过程，并采用Q-learning方案来优化CN调度策略。为降低学习复杂度，我们提出了一种新型的图诱导CN聚类方法，以这种方式对状态空间进行分区，使聚类之间的依赖性最小化。结果表明，与文献中的其他解码方法相比，新的强化学习方法不仅显著提高了解码性能，而且在模型被学习后，也大幅降低了解码复杂度。

54.《BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning》
- 关键词：imitation learning
- 在batch DRL设置中，常用的off-policy DRL算法的性能可能会很差，有时甚至根本无法学习。 在本文中，我们提出了一种新算法——最佳动作模仿学习（BAIL）。与许多off-policy DRL算法不同，该算法不涉及在动作空间上最大化Q函数。 BAIL在追求简单性的同时也追求性能，它首先从一批动作中选择它认为对其对应的状态是高绩效的动作，然后使用这些状态动作对使用模仿学习来训练一个策略网络。虽然BAIL很简单，但我们证明了BAIL在Mujoco基准上达到了最先进的性能。

55.《Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes》
- 关键词：meta learning、Continuously learning、Gaussian、nonstationarity
- 在元学习和持续学习中，持续学习以有限的经验来解决未见过的任务已经被广泛追求，但同时我们需要注意一些限制性的假设，如可获得的任务分布、独立和相同分布的任务以及明确的任务划分。然而，现实世界中的物理任务经常违反这些假设，导致性能下降。本文提出了一种基于持续在线模型的强化学习方法，它不需要预先训练来解决任务边界未知的任务无关问题。我们保持专家的混合来处理非稳态性，并用高斯过程来表示每种不同类型的动态，以有效利用收集到的数据和表达模型的不确定性。我们提出了一个过渡先验来考虑流数据的时间依赖性，并通过顺序变分推断在线更新混合物。我们的方法通过为从未见过的动态生成新的模型，并为以前见过的动态重用旧模型，可靠地处理了任务分布的转变。

56.《On Reward-Free Reinforcement Learning with Linear Function Approximation》
- 关键词：Reward、 Function Approximation

57.《Near-Optimal Reinforcement Learning with Self-Play》
- 关键词：game theory

58.《Robust Multi-Agent Reinforcement Learning with Model Uncertainty》
- 关键词：MARL、Robust

59.《Towards Minimax Optimal Reinforcement Learning in Factored Markov Decision Processes》
- 关键词：FMDPs、Minimax

60.《Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward》
- 关键词：MARL、Scale

61.《Constrained episodic reinforcement learning in concave-convex and knapsack settings》
- 关键词：constrained RL、combinatorial optimization
- 我们提出了一种用于带约束的表格式episode RL算法。 对于具有凹形奖励和凸形约束的设置以及具有硬约束（背包）的设置，我们提供了具有强大理论保障的模块化分析。 先前在约束强化学习中的大多数工作都局限于线性约束，而其余工作则集中在可行性问题或单个episode的设置上。 我们的实验表明，在现有的约束episode环境中，新算法明显优于以往方法。

62.《Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation》
- 关键词：Efficient、Low-Rank Matrix Estimation

63.《Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning》
- 未找到文章

64.《Cooperative Heterogeneous Deep Reinforcement Learning》
- 关键词：heterogeneous agents, cooperation

65.《Implicit Distributional Reinforcement Learning》
- 关键词：Distributional

66.《Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization》
- 关键词：Exploration、Inverse Reinforcement Learning

67.《EPOC: A Provably Correct Policy Gradient Approach to Reinforcement Learning》
- 关键词：Policy Gradient

68.《Provably Efficient Reinforcement Learning with Kernel and Neural Function Approximations》
- 关键词：kernel、Function Approximation

69.《Decoupled Policy Gradient Methods for Competitive Reinforcement Learning》
- 关键词：Decoupled Policy Gradient、Competitive

70.《Upper Confidence Primal-Dual Reinforcement Learning for CMDP with Adversarial Loss》
- 关键词：constrained RL、CMDP、Primal-Dual、Upper Confidence、Adversarial、safe RL
- 我们考虑episodic随机约束马尔科夫决策过程（CMDP）的在线学习，它在确保强化学习的安全性方面起着核心作用。其中，损失函数可在各个episodes中任意变化，接收到的损失和预算消耗都会在每个episode结束时被揭示。以往的工作是在限制性假设（即马尔科夫决策过程（MDP）的过渡模型是先验已知的）下解决此问题的，并且建立的后悔界取决于状态空间S和行动空间A的cardinality。在这项工作中，我们提出了一种新的upper confidence primal-dual算法，它只需要从过渡模型中采样的轨迹。我们将拉格朗日乘子过程的新的高概率漂移分析融入到著名的upper confidence强化学习的后悔分析中，证明了 “面对不确定性时的乐观 “在约束在线学习中的力量。

71.《Improving Generalization in Reinforcement Learning with Mixture Regularization》
- 关键词：Generalization、Regularization

72.《A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning》
- 关键词：MARL、game theory、resource management

73.《Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games》
- 关键词：representation、 Hierarchical Attention、 Text-based Games

74.《Robust Reinforcement Learning via Adversarial training with Langevin Dynamics》
- 关键词：Robust RL、Adversarial、 Langevin Dynamics

75.《Interferobot: aligning an optical interferometer by a reinforcement learning agent》
- 关键词：align、robot、domain randomizations （光学干涉实验）

76.《Reinforcement Learning for Control with Multiple Frequencies》
- 未找到文章

77.《Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret》
- 关键词：Risk、uncertainty
- 我们研究具有未知transition kernels的episodic马尔可夫决策过程中的风险敏感强化学习，目标是在指数效用的风险度量下优化总回报。 我们提出了两种可证明有效的无模型算法——风险敏感值迭代（RSVI）和风险敏感Q学习（RSQ）。 这些算法在面对不确定性时实现了一种风险敏感型乐观主义的形式，它同时适用于寻求风险和规避风险的探索方式。

78.《Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation》
- 关键词：Expert-Supervised、Offline Policy Learning and Evaluation

79.《Dynamic allocation of limited memory resources in reinforcement learning》
- 关键词：memory budget、Dynamic allocation

80.《AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control》
- 关键词：Attention-Based、Traffic Signal Control

81.《Sample-Efficient Reinforcement Learning of Undercomplete POMDPs》
- 关键词：Efficient、Undercomplete POMDPs

82.《RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning》
- 关键词：部分可观察性是RL落地的一大挑战，这要求智能体保持记忆，推断潜在状态并将过去的信息整合到探索中。 这项挑战导致了许多用于学习一般部分可观察的马尔可夫决策过程（POMDP）的计算和statistical hardness结果。 这项工作表明，这些hardness壁垒并不排除对POMDP丰富而有趣的子类进行有效的强化学习。 特别地，我们提出了一种样本有效的OOM-UCB算法，用于episodic finite的不完全POMDP，其观测数大于潜在状态数，并且探索对于学习至关重要，因此可将我们的结果与先前的研究区分开。 作为一个有趣的特例，我们还为具有确定性状态转换的POMDP提供了一种计算和统计有效的算法。

83.《A local temporal difference code for distributional reinforcement learning》
- 关键词：TD、distributional

84.《The Value Equivalence Principle for Model-Based Reinforcement Learning》
- 关键词：Model-Based RL、Value Equivalence Principle

85.《Steady State Analysis of Episodic Reinforcement Learning》
- 关键词：Episodic RL、Steady State Analysis

86.《Information-theoretic Task Selection for Meta-Reinforcement Learning》
- 关键词：Information theory、meta RL、Task Selection

87.《A Unifying View of Optimism in Episodic Reinforcement Learning》
- 关键词：Optimism、Episodic RL

88.《Accelerating Reinforcement Learning through GPU Atari Emulation》
- 关键词：GPU、Emulation、efficient

89.《Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations》
- 关键词：Robust、Adversarial

90.《Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning》
- 关键词：Model-Based RL、Imagination

91.《Adaptive Discretization for Model-Based Reinforcement Learning》
- 关键词：Adaptive、Discretization、 Model-Based RL

92.《Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration》
- 关键词：Batch RL、 Off-Policy

93.《Provably adaptive reinforcement learning in metric spaces》
- 关键词：adaptive、metric spaces

94.《Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model》
- 关键词：Latent Variable Model、Stochastic

95.《Inverse Reinforcement Learning from a Gradient-based Learner》
- 关键词：Inverse RL、gradient