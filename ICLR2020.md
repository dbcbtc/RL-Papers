- 创新模式总结：
    - （1）将某个问题首次使用强化学习进行建模【】
    - （2）将某个已经使用了RL方法的应用或理论问题进行问题场景细分（针对细分场景的问题开展研究）【】
    - （3）老问题首次引入某种新技术或提出新方法或改进【1】
    - （4）针对老问题给出理论研究结果，例如bound【】
- 技术类型总结：
    - （1）有模型 and 无模型 
    - （2）单agent and 多agent 
    - （3）层次的 
    - （4）确定性 and 不确定性  
    - （5）元强化学习 
    - （6）小规模 and 大规模 
    - （7）迁移适应能力
- 技术类型总结（多智能体）：
    - （8）中心化 and 去中心化 
    - （9）合作 
    - （10）通信理论
- 关键词：模拟器【】、专家信息展示【】、大规模【】、稳定性鲁棒性不确定性【】、元强化学习【】、高效探索采样【1】、监督和强化混合模型、对抗攻击【】、逐步分层解决【1】、序列生成【】、复杂对抗博弈【】、多智能体合作【】、模仿强化学习【】、多智能体博弈【】、算法理论研究【】、动作空间裁剪【】、迁移强化学习【】、通用游戏agent【】、非马尔科夫奖励【】、图划分【】、好奇心探索【】、主动学习【】、世界模型【】、自监督强化学习【】、模型误差【】、模型泛化性【】、模拟仿真【】、采样复杂度【】、环境变化策略适应【】
- 应用关键词：图上组合优化【】、交通灯【】、智能操盘交易【】、图像编辑【】、人类姿态评估【】、事件摘要【】、句子配对【】、基于文本的游戏【】、图像处理【】、对话生成【】、基于DAG图的资源分配【】、自然语言生成【】、恶意软件检测【】、政府决策【】

**1.Dynamics-Aware Unsupervised Skill Discovery**
- 链接 : <https://openreview.net/pdf?id=HJgLZR4KvH>
- 作者 : Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman
- 单位 : Google Brain
- 摘要：传统上，基于模型的强化学习（MBRL）旨在学习环境动力学的全局模型。一个好的模型可以潜在地使计划算法生成各种各样的行为并解决各种任务。但是，要为复杂的动力学系统学习准确的模型仍然很困难，即使如此，该模型也可能无法在训练它的状态分布之外很好地推广。在这项工作中，我们结合了基于模型和无模型的学习方法，使基于模型的计划变得容易。为此，我们旨在回答这个问题：我们如何发现其结果易于预测的技能？我们提出了一种无监督的学习算法，即“动态感知技能发现（DADS）”，该算法同时发现可预测的行为并了解其动态。我们的方法可以利用连续的技能空间，理论上使我们能够学习无数种行为甚至对于连续状态空间。我们证明，在学习潜在空间中的零样本规划显着优于标准MBRL和无模型的目标条件RL，可以处理稀疏任务，并且与以前的分层RL方法相比有显着改进对于无监督的技能发现
- 创新模式：老问题首次引入某种新技术或提出新方法或改进
- 技术总结：有模型，单agent，层次的 
- 关键词：高效探索采样，逐步分层解决，无监督强化学习（新增），稀疏奖励（新增）
- 源代码：<https://github.com/google-research/dads>
- 贡献：我们工作的关键贡献是建立在基于互信息的探索基础上的无监督强化学习算法DADS。 我们证明了我们的目标可以在连续的空间中嵌入学习过的原语，这使我们能够学习大量多样的技能。 至关重要的是，我们的算法还学会了对技能动态进行建模，从而可以将基于模型的计划算法用于下游任务。 我们采用了传统的模型预测控制算法来在原语空间中进行计划，并证明我们可以组合学习到的原语来解决下游任务，而无需任何额外的训练。

**2.Contrastive Learning of Structured World Models**
- 链接 | https://openreview.net/pdf?id=H1gax6VtDB
- 作者 | Thomas Kipf, Elise van der Pol, Max Welling
- 单位 | University of Amsterdam
- 负责 | 沈硕

**3.Implementation Matters in Deep RL: A Case Study on PPO and TRPO**
- 链接 | https://openreview.net/pdf?id=r1etN1rtPB
- 作者 | Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
- 负责 | 穆亭江

**4.GenDICE: Generalized Offline Estimation of Stationary Values**
- 链接 | https://openreview.net/pdf?id=HkxlcnVFwB
- 作者 | Ruiyi Zhang, Bo Dai, Lihong Li, Dale Schuurmans
- 单位 | Duke University; Google Brain
- 负责 | 蒋檄文


**5.Causal Discovery with Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1g2skStPB
- 作者 | Shengyu Zhu, Ignavier Ng, Zhitang Chen
- Huawei Noah’s Ark Lab; University of Toronto
- 负责 | 刘辰宇


**6.Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?**
- 链接 | https://openreview.net/pdf?id=r1genAVKPB
- 作者 | Simon S. Du, Sham M. Kakade, Ruosong Wang, Lin F. Yang
- 单位 | University of Washington; Carnegie Mellon University; University of California, Los Angles
- 负责 | 刘辰宇

**7.Harnessing Structures for Value-Based Planning and Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rklHqRVKvH
- 作者 | Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi
- 单位 | MIT
- 负责 | 刘辰宇

**8.Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency**
- 链接 | https://openreview.net/pdf?id=SJgzLkBKPB
- 作者 | Piyush Gupta, Nikaash Puri, Sukriti Verma, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, Sameer Singh
- 单位 | Adobe;
- 负责 | 郭智慧

**9.Meta-Q-Learning**
- 链接 | https://openreview.net/pdf?id=SJeD3CEFPH
- 作者 | Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J. Smola
- Amazon; University of Pennsylvania
- 负责 | 胡小波

**10.Discriminative Particle Filter Reinforcement Learning for Complex Partial observations**
- 链接 | https://openreview.net/pdf?id=HJl8_eHYvS
- 作者 | Xiao Ma, Peter Karkus, David Hsu, Wee Sun Lee, Nan Ye
- 单位 | National Unviersity of Singapore; The University of Queesland
- 负责 | 蒋檄文

**11.Disagreement-Regularized Imitation Learning**
- 链接 | https://openreview.net/pdf?id=rkgbYyHtwB
- 作者 | Kiante Brantley, Wen Sun, Mikael Henaff
- 单位 | University of Maryland; Microsoft Research
- 负责 | 郭智慧
- 摘要 | 我们提出了一个简单而有效的算法来解决模仿学习中的协变量转移问题。它通过对专家演示数据的策略集合进行培训，并使用他们预测的方差作为成本，而RL和监督行为克隆成本使其最小化。与对抗的模仿方法不同，它使用固定的优化很简单的奖励函数。我们证明了算法的遗憾界，即在时间范围内是线性的，再乘以一个系数，对于某些行为克隆失败的问题，这个系数是低的。我们在基于多个像素的Atari环境和连续控制任务中对我们的算法进行了经验评估，结果显示，它匹配或显著优于行为克隆和生成性对抗模仿学习。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 模仿学习
- 关键词 | 监督和强化混合模型 模仿强化学习
- 源代码 | 没找到
- 贡献 | 解决了协变量转变的问题

**12.Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation**
- 链接 | https://openreview.net/pdf?id=S1glGANtDr
- 作者 | Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, Qiang Liu
- 单位 | The University of Texas at Austin; Google Research
- 负责 | 蒋檄文

**13.SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference**
- 链接 | https://openreview.net/pdf?id=rkgvXlrKwH
- 作者 | Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, Marcin Michalski
- 单位 | Google Research
- 负责 | 韩帅

**14.The Ingredients of Real World Robotic Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rJe2syrtvS
- 作者 | Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, Sergey Levine
- 负责 | 赵亮

**15.Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search**
- 链接 | https://openreview.net/pdf?id=BJlQtJSKDB
- 作者 | Anji Liu, Jianshu Chen, Mingze Yu, Yu Zhai, Xuewen Zhou, Ji Liu
- 单位 | Tencent AI Lab
- 负责 | 林晨

**16.Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization**
- 链接 | https://openreview.net/pdf?id=ryeYpJSKwr
- 作者 | Michael Volpp, Lukas P. Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, Christian Daniel
- 负责 | 林晨

**17.A Closer Look at Deep Policy Gradients**
- 链接 | https://openreview.net/pdf?id=ryxdEkHtPS
- 作者 | Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
- 负责 | 胡小波

**18.Fast Task Inference with Variational Intrinsic Successor Features**
- 链接 | https://openreview.net/pdf?id=BJeAHkrYDS
- 作者 | Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, Volodymyr Mnih
- 单位 | DeepMind
- 负责 | 蒋檄文

**19.Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees**
- 链接 | https://openreview.net/pdf?id=rJgJDAVKvB
- 作者 | Binghong Chen, Bo Dai, Qinjie Lin, Guo Ye, Han Liu, Le Song
- 单位 | Georgia Institute of Technology; Google Research; Northwestern University
- 负责 | 刘辰宇

**20.Dream to Control: Learning Behaviors by Latent Imagination**
- 链接: https://openreview.net/pdf?id=S1lOTC4tDS
- 作者: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi
- 单位: University of Toronto; DeepMind; Google Brain
- 摘要: 学习一个总结了智能体经验的世界模型有助于学习复杂的行为。深度学习使从高维的输入中学习环境模型成为可能，同时也有很多潜在的方法可以从环境模型中学习行为。我们提出了Dreamer，一个强化学习智能体从图像输入中通过隐空间想象解决长视野任务。我们通过高效的从学到的环境模型中想象出来的轨迹序列中对价值函数进行梯度传播来学习策略。在20个图像输入的任务中，Dreamer在样本效率、计算时间、最终结果上都超过了现有的方法。
- 创新模式: 老问题首次引入某种新技术或提出新方法或改进，基于作者
- 技术总结: 基于模型的（world model）, 单智能体 ,确定性环境（mujoco）
- 关键词: 状态隐空间（latent dynamics）中控制， 多步reward（n-step TD）,Actor-Critic
- 源代码以及其他资料: https://danijar.com/project/dreamer/
- 贡献: 通过在状态隐空间上想象，相比其他model base agent拥有更长的视野（long-horizon）。在DeepMind Control Suite 的20个任务上运行，在样本效率，计算时间、最终表现上都超过了现有的方法

**21.Making Efficient Use of Demonstrations to Solve Hard Exploration Problems**
- 链接: https://openreview.net/pdf?id=SygKyeHKDH
- 作者: Caglar Gulcehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team
- 单位: DeepMind
- 摘要: 本文介绍了R2D3算法( RECURRENT REPLAY DISTRIBUTED DQN FROM DEMONSTRATIONS)，训练出的智能体可以有效利用演示样本来解决在<u>初始条件高度可变</u>的<u>部分可观察环境</u>中的<u>探索困难</u>的问题。我们还引入了包含这三个属性的八个任务z组件，并证明了R2D3可以解决其他一些先进方法（无论有无演示）在几百亿个探索轨迹后仍不能获得一次成功的轨迹的任务。
- 创新模式: 老问题首次引入某种新技术或提出新方法或改进
- 技术总结: 无模型，单智能体
- 关键词: 演示样本，探索，循环网络(rnn)，部分可观，环境初始高度可变
- 源代码: 无
- 贡献:在R2D2算法的基础上引进domo-ration因子，融合演示样本与agent探索经验，能够有效提取出演示中的有效信息，在其基础上提高探索效果。

**22.Intrinsic Motivation for Encouraging Synergistic Behavior**
- 链接 | https://openreview.net/pdf?id=SJleNCNtDH
- 作者 | Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, Abhinav Gupta
- 单位 | MIT; Facebook AI Research
- 负责 | 赵亮

**23.SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards**
- 链接 | https://openreview.net/pdf?id=S1xKd24twB
- 作者 | Siddharth Reddy, Anca D. Dragan, Sergey Levine
- 单位 | UC Berkeley
- 负责 | 刘辰宇

**24.Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives**
- 链接 | https://openreview.net/pdf?id=ryxgJTEYDr
- 作者 | Anirudh Goyal, Shagun Sodhani, Jonathan Binas, Xue Bin Peng, Sergey Levine, Yoshua Bengio
- 负责 | 赵亮

**25.Multi-Agent Interactions Modeling with Correlated Policies**
- 链接 | https://openreview.net/pdf?id=B1gZV1HYvS
- 作者 | Minghuan Liu, Ming Zhou, Weinan Zhang, Yuzheng Zhuang, Jun Wang, Wulong Liu, Yong Yu
- 单位 | Shanghai Jiaotong University; Huawei Noah’s Ark Lab
- 负责 | 许鑫悦

**26.Influence-Based Multi-Agent Exploration**
- 链接 | https://openreview.net/pdf?id=BJgy96EYvr
- 作者 | Tonghan Wang, Jianhao Wang, Yi Wu, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 胡小波

**27.Learning the Arrow of Time for Problems in Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rylJkpEtwS
- 作者 | Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, Yoshua Bengio
- 单位 | MILA
- 负责 | 穆亭江

**28.AMRL: Aggregated Memory For Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=Bkl7bREtDr
- 作者 | Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, Katja Hofmann
- 单位 | Microsoft Research
- 负责 | 林晨

**29.Model Based Reinforcement Learning for Atari**
- 链接：https://openreview.net/pdf?id=S1xCPJHtDB
- 作者：Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski
— 单位：Google Brain
- 摘要：Model Free强化学习可以在复杂的环境下学习有效的策略，即使是Atari游戏中以图像作为输入。然而这通常需要大量的环境交互，通常比人类学习同样的游戏所需的多的多。为什么人类可以学的这么快？部分原因可能是人类能够学习游戏是如何运作的，然后预测某个动作可能产生的后果。这篇文章中我们探索视频图像预测模型如何让agent使用比model free方法更少的样本解决Atari游戏问题。我们提出SimPLe，一种基于视频图像输入的完全Model Base方法，同时提出几种Model 网络结构。我们在一系列Atari游戏中评估了SimPLe，同时环境交互数量限制在100K这个较低的数量上，等同于在游戏中运行两小时。在大部分游戏中表现都超过了Model Free方法，在部分游戏中效果超过了一个量级。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进
- 技术总结：有模型的，单agent
- 关键词：Atari，图像输入，Model Base, Sample Efficiency
- 源代码：https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/rl
- 贡献：SimPLe通过图像输入学到了高效的策略来玩Atari游戏，在大部分的游戏中只需不到100K样本交互，2小时的游戏时间。Model的隐变量部分能有效处理随机性。

**30.Variational Recurrent Models for Solving Partially Observable Control Tasks**
- 链接 | https://openreview.net/pdf?id=r1lL4a4tDB
- 作者 | Dongqi Han, Kenji Doya, Jun Tani
- 负责 | 宋明惠

**31.Sample Efficient Policy Gradient Methods with Recursive Variance Reduction**
- 链接 | https://openreview.net/pdf?id=HJlxIJBFDr
- 作者 | Pan Xu, Felicia Gao, Quanquan Gu
- 单位 | University of California, Los Angeles
- 负责 | 王善锐

**32.Exploring Model-based Planning with Policy Networks**
- 链接 | https://openreview.net/pdf?id=H1exf64KwH
- 作者 | Tingwu Wang, Jimmy Ba
- 单位 | University of Toronto; Vector Institute
- 负责 | 沈硕

**33.Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation**
- 链接 | https://openreview.net/pdf?id=HygnDhEtvr
- 作者 | Yu Chen, Lingfei Wu, Mohammed J. Zaki
- 单位 | Rensselaer Polytechnic Institute; IBM Research
- 负责 | 穆亭江

**34.RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments**
- 链接 | https://openreview.net/pdf?id=rkg-TJBFPB
- 作者 | Roberta Raileanu, Tim Rocktäschel
- 单位 | New York University; University College London
- 负责 | 宋明惠

**35.Learning Expensive Coordination: An Event-Based Deep RL Approach**
- 链接 | https://openreview.net/pdf?id=ryeG924twB
- 作者 | Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, Bo An
- 单位 | Nanyang Technological University; Sun Yat-sen University
- 负责 | 王善锐

**36.Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=SJxbHkrKDH
- 作者 | Qian Long, Zihan Zhou, Abhinav Gupta, Fei Fang, Yi Wu, Xiaolong Wang
- 单位 | CMU; OpenAI; Facebook AI Research; SJTU; UCSD
- 负责 | 赵亮

**37.Making Sense of Reinforcement Learning and Probabilistic Inference**
- 链接 | https://openreview.net/pdf?id=S1xitgHtvS
- 作者 | Brendan O’Donoghue, Ian Osband, Catalin Ionescu
- 负责 | 刘辰宇

**38.Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs**
- 链接 | https://openreview.net/pdf?id=rkxDoJBYPB
- 作者 | Aditya Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, Oriol Vinyals
- 单位 | Google Research; DeepMind;
- 负责 | 赵亮

**39.Never Give Up: Learning Directed Exploration Strategies**
- 链接 | https://openreview.net/pdf?id=Sye57xStvB
- 作者 | Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, Charles Blundell
- 单位 | DeepMind
- 负责 | 王善锐

**40.Robust Reinforcement Learning for Continuous Control with Model Misspecification**
- 链接 | https://openreview.net/pdf?id=HJgC60EtwB
- 作者 | Daniel J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, Martin Riedmiller
- 单位 | DeepMind
- 负责 | 沈硕

**41.Synthesizing Programmatic Policies that Inductively Generalize**
- 链接 | https://openreview.net/pdf?id=S1l8oANFDH
- 作者 | Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, Armando Solar-Lezama
- 单位 | MIT; University of Pennsylvania
- 负责 | 宋明惠

**42.Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation**
- 链接 | https://openreview.net/pdf?id=r1lOgyrKDS
- 作者 | Xinjie Fan, Yizhe Zhang, Zhendong Wang, Mingyuan Zhou
- 单位 | University of Texas at Austin; Microsoft Research; Columbia University
- 负责 | 蒋檄文

**43.Improving Generalization in Meta Reinforcement Learning using Learned Objectives**
- 链接 | https://openreview.net/pdf?id=S1evHerYPr
- 作者 | Louis Kirsch, Sjoerd van Steenkiste, Juergen Schmidhuber
- 负责 | 郭智慧

**44.Single Episode Policy Transfer in Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rJeQoCNYDS
- 作者 | Jiachen Yang, Brenden Petersen, Hongyuan Zha, Daniel Faissol
- 单位 | Georgia Institute of Technology
- 负责 | 王翔森

**45.DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames**
- 链接 | https://openreview.net/pdf?id=H1gX8C4YPr
- 作者 | Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra
- 单位 | Georgia Institute of Technology; Facebook AI Research
- 负责 | 赵亮

**46.Geometric Insights into the Convergence of Nonlinear TD Learning**
- 链接 | https://openreview.net/pdf?id=SJezGp4YPr
- 作者 | David Brandfonbrener, Joan Bruna
- 单位 | New York University
- 负责 | 蒋檄文

**47.Dynamics-Aware Embeddings**
- 链接 | https://openreview.net/pdf?id=BJgZGeHFPH
- 作者 | William Whitney, Rajat Agarwal, Kyunghyun Cho, Abhinav Gupta
- 单位 | New York University; Carnegie Mellon University; Facebook AI Research
- 负责 |  穆亭江

**48.Reanalysis of Variance Reduced Temporal Difference Learning**
- 链接 | https://openreview.net/pdf?id=S1ly10EKDS
- 作者 | Tengyu Xu, Zhe Wang, Yi Zhou, Yingbin Liang
- 单位 | Ohio State University; University of Utah
- 负责 | 蒋檄文

**49.Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP**
- 链接 | https://openreview.net/pdf?id=BkglSTNFDB
- 作者 | Yuanhao Wang, Kefan Dong, Xiaoyu Chen, Liwei Wang
- 单位 | Tsinghua University; Peking University
- 负责 | 胡小波

**50.Automated curriculum generation through setter-solver interactions**
- 链接 | https://openreview.net/pdf?id=H1e0Wp4KvH
- 作者 | Sebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, Timothy Lillicrap
- 单位 | DeepMind

**51.Optimistic Exploration even with a Pessimistic Initialisation**
- 链接 | https://openreview.net/pdf?id=r1xGP6VYwH
- 作者 | Tabish Rashid, Bei Peng, Wendelin Boehmer, Shimon Whiteson
- 单位 | University of Oxford
- 负责 | 王善锐

**52.Multi-agent Reinforcement Learning for Networked System Control**
- 链接 | https://openreview.net/pdf?id=Syx7A3NFvH
- 作者 | Tianshu Chu, Sandeep Chinchali, Sachin Katti
- 单位 | Stanford University
- 负责 | 胡小波

**53.A Learning-based Iterative Method for Solving Vehicle Routing Problems**
- 链接 | https://openreview.net/pdf?id=BJe1334YDH
- 作者 | Hao Lu, Xingwen Zhang, Shuang Yang
- 单位 | Princeton University
- 负责 | 穆亭江

**54.Sharing Knowledge in Multi-Task Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rkgpv2VFvr
- 作者 | Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters
- 负责 | 胡小波

**55.RTFM: Generalising to New Environment Dynamics via Reading**
- 链接 | https://openreview.net/pdf?id=SJgob6NKvH
- 作者 | Victor Zhong, Tim Rocktäschel, Edward Grefenstette
- 单位 | University of Washington; University College London; Facebook AI Research
- 负责 | 穆亭江

**56.Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies**
- 链接 | https://openreview.net/pdf?id=HkgsWxrtPB
- 作者 | Sungryull Sohn, Hyunjae Woo, Jongwook Choi, Honglak Lee
- 单位 | University of Michigan; Google Brain
- 负责 | 王翔森

**57.Projection-Based Constrained Policy Optimization**
- 链接 | https://openreview.net/pdf?id=rke3TJrtPS
- 作者 | Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, Peter J. Ramadge
- 单位 | Princeton University;

**58.Graph Constrained Reinforcement Learning for Natural Language Action Spaces**
- 链接 | https://openreview.net/pdf?id=B1x6w0EtwH
- 作者 | Prithviraj Ammanabrolu, Matthew Hausknecht
- 单位 | Georgia Institute of Technology; Microsoft Researc
- 负责 | 宋明惠

**59.V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control**
- 链接 | https://openreview.net/pdf?id=SylOlp4FvH
- 作者 | H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin Riedmiller, Matthew M. Botvinick
- 单位 | DeepMind
- 负责 | 赵亮

**60.Thinking While Moving: Deep Reinforcement Learning with Concurrent Control**
- 链接 | https://openreview.net/pdf?id=Hke0V1rKPS
- 作者 | Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman, Alexander Herzog
- 单位 | Nanyang Technological University; MILA
- 负责 | 宋明惠

**61.Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rke7geHtwH
- 作者 | Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, Martin Riedmiller
- 单位 | DeepMind
- 负责 | 林晨

**62.Imitation Learning via Off-Policy Distribution Matching**
- 链接 | https://openreview.net/pdf?id=Hyg-JC4FDr
- 作者 | Ilya Kostrikov, Ofir Nachum, Jonathan Tompson
- 单位 | Google Research
- 负责 | 郭智慧

**63.Adversarial AutoAugment**
- 链接 | https://openreview.net/pdf?id=ByxdUySKvS
- 作者 | Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong
- 负责 | 韩帅

**64.Option Discovery using Deep Skill Chaining**
- 链接 | https://openreview.net/pdf?id=B1gqipNYwH
- 作者 | Akhil Bagaria, George Konidaris
- 单位 | Brown University
- 负责 | 王善锐
- 摘要：自主地发现时间上的拓展动作(temporally extended actions，也就是上层的输出)，或技巧(skill)，长久以来一直是分层强化学习的目标。我们提出了一种新的算法，该算法通过将技巧链接(skill chaining)与深度神经网络相结合，以此在高维、连续的问题域中，自动地发现技巧。最终得到的算法，deep skill chaining，构建skill，让其可以执行一个skill，进而让智能体去执行另一个（个人理解为skill之间是环环相扣的，因此叫skill chaining）。最终我们发现deep skill chaining在具有挑战性的连续控制任务中，不仅可以打败无分层结构的智能体，也优于其他基于skill discovery的分层强化学习算法。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进，将skill chaining和非线性函数近似相结合，引入到skill discovery的分层强化学习中去，用于解决高维的连续控制问题。
- 技术总结：无模型，单agent，层次的
- 关键词：skill chainig，分层强化学习，option框架
- 源代码：https://github.com/deep-skill-chaining/deep-skill-chaining
- 贡献：我们提出了新的skill discovery算法，DSC，用于解决高维的、目标导向的任务，并且表现明显由于普通的RL算法以及其他著名的分层方法。就我们所知，DSC是首个不把option的数量作为一个固定且代价高昂的超参数的深度option discovery算法；同时，其他深度option discovery算法都致力于展现其相较于不分层算法，在某个任务中的提高，而我们则展示了对任务分层的必要性。

**65.State-only Imitation with Transition Dynamics Mismatch**
- 链接 | https://openreview.net/pdf?id=HJgLLyrYwB
- 作者 | Tanmay Gangwani, Jian Peng
- 单位 | University of Illinois, Urbana-Champaign
- 负责 | 郭智慧

**66.The Gambler’s Problem and Beyond**
- 链接 | https://openreview.net/pdf?id=HyxnMyBKwB
- 作者 | Baoxiang Wang, Shuai Li, Jiajin Li, Siu On Chan
- 单位 | Chinese University of Hong Kong; Shanghai Jiao Tong University
- 负责 | 王善锐
- 摘要：我们分析了赌博机问题，一个简单的强化学习问题，在该问题中，赌博机有一定概率令其赌注加倍或者丢失，直到达到目标。该问题最早是作为一个简单的例子，在《Reinforcement Learning: An Introduction》一书中被介绍，其中提到了一种有趣的最优值函数模式，带有高频成分（high-frequency components）和重复非平滑点（repeating non-smooth points），但书中并没有给出进一步的解释。我们同时为离散动作问题和连续动作问题给出了最优值函数的具体形式。尽管它看起来比较简单，但值函数是无法控制的（pathlogical）：分形（fractal，分数维度）、自相似性（self-similar）、倒数取0或者无穷大（derivative taking either zero or infinity）、任何间隔都不平滑（not smooth on any interval）、无法写成初等函数的组合（not written as elementary functions）。事实上它是一种广义的康托函数，这种函数拥有迄今为止尚未发现的复杂性。我们的分析可以对提升值函数近似、策略梯度算法、Q-learning，在应用和实现中有更深刻的理解。
- 创新模式：针对老问题给出理论研究结果
- 技术总结：无模型，单agent，
- 关键词：赌博机问题，最优值函数，MDP
- 源代码：无
- 贡献：我们给出了不同设置下，赌博机问题的完整解。我们展示了最优值函数非常复杂，甚至是无法控制的，尽管看起来简单，但这些结果在之前的研究中都没有被指明。我们的贡献是理论上的发现和实现。其结果值得在社区中展开探讨。通过对赌博机问题指出来的，现在的大多数强化学习算法可能低估了复杂性。

**67.Structured Object-Aware Physics Prediction for Video Modeling and Planning**
- 链接 | https://openreview.net/pdf?id=B1e-kxSKDH
- 作者 | Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting
- 负责 | 沈硕

**68.Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery**
- 链接 | https://openreview.net/pdf?id=H1lmhaVtvr
- 作者 | Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine
- 负责 | 韩帅

**69.Exploration in Reinforcement Learning with Deep Covering Options**
- 链接 | https://openreview.net/pdf?id=SkeIyaVtwB
- 作者 | Yuu Jinnai, Jee Won Park, Marlos C. Machado, George Konidaris
- 单位 | Brown University; Google Brain
- 负责 | 胡小波

**70.CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1lEX04tPr
- 作者 | Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, Hongyuan Zha
- 单位 | Georgia Institute of Technology
- 负责 | 许鑫悦

**71.Learning to Coordinate Manipulation Skills via Skill Behavior Diversification**
- 链接 | https://openreview.net/pdf?id=ryxB2lBtvH
- 作者 | Youngwoon Lee, Jingyun Yang, Joseph J. Lim
- 单位 | University of Southern California
- 负责 | 王善锐

**72.Composing Task-Agnostic Policies with Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=H1ezFREtwH
- 作者 | Ahmed H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, Michael C. Yip
- 单位 | UC San Diego; University of Washington
- 负责 | 韩帅

**73.Frequency-based Search-control in Dyna**
- 链接 | https://openreview.net/pdf?id=B1gskyStwr
- 作者 | Yangchen Pan, Jincheng Mei, Amir-massoud Farahmand
- 单位 | University of Alberta; Vector Institute; University of Toronto
- 负责 | 沈硕

**74.Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1ltg1rFDS
- 作者 | Ali Mousavi, Lihong Li, Qiang Liu, Denny Zhou
- 单位 | Google Research; University of Texas, Austin
- 负责 | 韩帅

**75.CAQL: Continuous Action Q-Learning**
- 链接 | https://openreview.net/pdf?id=BkxXe0Etwr
- 作者 | Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, Craig Boutilier
- 单位 | Google Research
- 负责 | 王善锐

**76.Reinforced active learning for image segmentation**
- 链接 | https://openreview.net/pdf?id=SkgC6TNFvr
- 作者 | Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J. Pal
- 单位 | MILA; Element AI
- 负责 | 郭智慧
- 摘要 | 基于学习的方法对于语义分割有两个固有的挑战。首先，获取像素级标签是费时费力的。第二，真实的分割数据集是高度不平衡的：一些类别远远比其他类别丰富的多，使得性能偏向于最多表现的种类。在本篇论文中，我们感兴趣的是将人类的标记工作集中在大数据池中的一个小子集上，使这种工作最小化，同时最大化一个保留集合上的分割模型的性能。我们提出了一种基于DRL的语义分割主动学习策略。一个agent学习一个策略从一个未标记的数据池中选择一个包含信息的小图片区域来进行标记，而不是整个图像区域。区域选择决策是根据所训练的分割模型的预测和不确定性做出的。我们的方法提出了一种主动学习的DQN新的改进，使它适应大规模的语义分割问题。我们在CamVid中测试概念证明，并在大规模Citycapes数据集中提供结果。在Cityscapes上，我们基于区域的DQN方法需要的额外标记数据比我们最具竞争力的baseline少大约30%，就能达到相同的性能。此外，我们发现，与baseline相比，我们的方法要求对代表性不足的类别使用更多的标签，从而提高它们的性能，并有助于减轻类别的不平衡。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 无模型 单agent 大规模 
- 关键词 | 大规模 主动学习 图像处理 DQN
- 源代码 | https://github.com/ArantxaCasanova/ralis
- 贡献 | 1、我们学习一个基于RL的函数来学习基于区域的主动学习图像分割。
         2、我们使用批处理模式的DQN来构建主动学习框架，它在每次主动学习迭代时并行标注多个区域(这是一种更有效的大规模数据集策略，与标准的迷你批处理梯度下降兼容)。
         3、我们测试了概念证明在CamVid数据集上，并且在Cityscapes数据集上提供了结果击败了最近最先进的BALD算法，一个被广泛应用的基于熵的选择准则和均匀采样基线。

**78.The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget**
- 链接 | https://openreview.net/pdf?id=Hye1kTVFDS
- 作者 | Anirudh Goyal, Yoshua Bengio, Matthew Botvinick, Sergey Levine
- 负责 | 王翔森

**79.Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation**
- 链接 | https://openreview.net/pdf?id=H1gzR2VKDH
- 作者 | Suraj Nair, Chelsea Finn
- 单位 | Stanford University; Google Brain
- 负责 | 宋明惠

**80.Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=BJliakStvH
- 作者 | Dexter R.R. Scobee, S. Shankar Sastry
- 单位 | UC Berkeley
- 负责 | 韩帅

**81.AutoQ: Automated Kernel-Wise Neural Network Quantization**
- 链接 | https://openreview.net/pdf?id=rygfnn4twS
- 作者 | Qian Lou, Feng Guo, Minje Kim, Lantao Liu, Lei Jiang.
- 负责 | 韩帅

**82.VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning**
- 链接 | https://openreview.net/pdf?id=Hkl9JlBYvr
- 作者 | Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson
- 单位 | University of Oxford; Microsoft Research
- 负责 | 林晨

**83.Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards**
- 链接 | https://openreview.net/pdf?id=SJg5J6NtDr
- 作者 | Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, Chelsea Finn
- 单位 | Google Brain; UC Berkeley; Stanford
- 负责 | 郭智慧

**84.Population-Guided Parallel Policy Search for Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rJeINp4KwH
- 作者 | Whiyoung Jung, Giseung Park, Youngchul Sung
- 负责 | 韩帅

**85.Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=HJgcvJBFvB
- 作者 | Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee
- 单位 | University of Michigan; Google Brain
- 负责 | 穆亭江

**86.On the Weaknesses of Reinforcement Learning for Neural Machine Translation**
- 链接 | https://openreview.net/pdf?id=H1eCw3EKvH
- 作者 | Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend
- 负责 | 王翔森

**87.State Alignment-based Imitation Learning**
- 链接 | https://openreview.net/pdf?id=rylrdxHFDr
- 作者 | Fangchen Liu, Zhan Ling, Tongzhou Mu, Hao Su
- 单位 | University of California San Diego
- 负责 | 郭智慧

**88.Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents**
- 链接 | https://openreview.net/pdf?id=rylvYaNYDH
- 作者 | Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal
- 单位 | University of Oxford; Element AI; MILA
- 负责 | 宋明惠

**89.Model-Augmented Actor-Critic: Backpropagating through Paths**
- 链接 | https://openreview.net/pdf?id=Skln2A4YDB
- 作者 | Ignasi Clavera, Yao Fu, Pieter Abbeel
- 负责 | 沈硕

**90.Behaviour Suite for Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rygf-kSYwH
- 作者 | Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt
- 单位 | DeepMind
- 负责 | 刘辰宇

**91.Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=BJluxREKDB
- 作者 | Gil Lederman, Markus Rabe, Sanjit Seshia, Edward A. Lee
- 单位 | UC Berkeley; Google Research
- 负责 | 林晨

**92.Maxmin Q-learning: Controlling the Estimation Bias of Q-learning**
- 链接 | https://openreview.net/pdf?id=Bkg0u3Etwr
- 作者 | Qingfeng Lan, Yangchen Pan, Alona Fyshe, Martha White
- 单位 | University of Alberta
- 负责 | 赵亮

**93.Hypermodels for Exploration**
- 链接 | https://openreview.net/pdf?id=ryx6WgStPB
- 作者 | Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, Benjamin Van Roy

**94.Sub-policy Adaptation for Hierarchical Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=ByeWogStDS
- 作者 | Alexander Li, Carlos Florensa, Ignasi Clavera, Pieter Abbeel
- 单位 | UC Berkeley
- 负责 | 穆亭江

**95.SVQN: Sequential Variational Soft Q-Learning Networks**
- 链接 | https://openreview.net/pdf?id=r1xPh2VtPB
- 作者 | Shiyu Huang, Hang Su, Jun Zhu, Ting Chen
- 单位 | Tsinghua University
- 负责 | 王翔森

**96.IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks**
- 链接 | https://openreview.net/pdf?id=BJeGlJStPr
- 作者 | Michael Luo, Jiahao Yao, Richard Liaw, Eric Liang, Ion Stoica
- 单位 | UC Berkeley
- 负责 | 蒋檄文

**97.Ranking Policy Gradient**
- 链接 | https://openreview.net/pdf?id=rJld3hEYvS
- 作者 | Kaixiang Lin, Jiayu Zhou
- 单位 | Michigan State University
- 负责 | 刘辰宇

**98.Model-based reinforcement learning for biological sequence design**
- 链接 | https://openreview.net/pdf?id=HklxbgBKvr
- 作者 | Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, Lucy Colwell
- 单位 | Google Research; Caltech
- 负责 | 胡小波

**99.Learning Nearly Decomposable Value Functions Via Communication Minimization**
- 链接 | https://openreview.net/pdf?id=HJx-3grYDB
- 作者 | Tonghan Wang, Jianhao Wang, Chongyi Zheng, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 王翔森

**100.Implementing Inductive bias for different navigation tasks through diverse RNN attrractors**
- 链接 | https://openreview.net/pdf?id=Byx4NkrtDS
- 作者 | Tie XU, Omri Barak
- 负责 | 宋明惠

**101.Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control**
- 链接 | https://openreview.net/pdf?id=SylL0krYPS
- 作者 | Tsui-Wei Weng, Krishnamurthy (Dj) Dvijotham, Jonathan Uesato, Kai Xiao, Sven Gowal, Robert Stanforth, Pushmeet Kohli
- 单位 | MIT; DeepMind
- 负责 | 林晨

**102.Learning Efficient Parameter Server Synchronization Policies for Distributed SGD**
- 链接 | https://openreview.net/pdf?id=rJxX8T4Kvr
- 作者 | Rong Zhu, Sheng Yang, Andreas Pfadler, Zhengping Qian, Jingren Zhou

**103.Episodic Reinforcement Learning with Associative Memory**
- 链接 | https://openreview.net/pdf?id=HkxjqxBYDB
- 作者 | Guangxiang Zhu, Zichuan Lin, Guangwen Yang, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 王翔森

**104.Logic and the 2-Simplicial Transformer**
- 链接 | https://openreview.net/pdf?id=rkecJ6VFvr
- 作者 | James Clift, Dmitry Doryn, Daniel Murfet, James Wallbridge
- 单位 | University of Melbourne

**105.Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rkl3m1BFDB
- 作者 | Akanksha Atrey, Kaleigh Clary, David Jensen
- 单位 | University of Massachusetts Amherst
- 负责 | 王翔森

**106.Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP**
- 链接 | https://openreview.net/pdf?id=S1xnXRVFwH
- 作者 | Haonan Yu, Sergey Edunov, Yuandong Tian, Ari S. Morcos
- 单位 | Facebook AI Research
- 负责 | 林晨
