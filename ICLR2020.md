- 创新模式总结：
    - （1）将某个问题首次使用强化学习进行建模【】
    - （2）将某个已经使用了RL方法的应用或理论问题进行问题场景细分（针对细分场景的问题开展研究）【】
    - （3）老问题首次引入某种新技术或提出新方法或改进【1】
    - （4）针对老问题给出理论研究结果，例如bound【】
- 技术类型总结：
    - （1）有模型 and 无模型 
    - （2）单agent and 多agent 
    - （3）层次的 
    - （4）确定性 and 不确定性  
    - （5）元强化学习 
    - （6）小规模 and 大规模 
    - （7）迁移适应能力
- 技术类型总结（多智能体）：
    - （8）中心化 and 去中心化 
    - （9）合作 
    - （10）通信理论
- 关键词：模拟器【】、专家信息展示【】、大规模【】、稳定性鲁棒性不确定性【】、元强化学习【】、高效探索采样【1】、监督和强化混合模型、对抗攻击【】、逐步分层解决【1】、序列生成【】、复杂对抗博弈【】、多智能体合作【】、模仿强化学习【】、多智能体博弈【】、算法理论研究【】、动作空间裁剪【】、迁移强化学习【】、通用游戏agent【】、非马尔科夫奖励【】、图划分【】、好奇心探索【】、主动学习【】、世界模型【】、自监督强化学习【】、模型误差【】、模型泛化性【】、模拟仿真【】、采样复杂度【】、环境变化策略适应【】
- 应用关键词：图上组合优化【】、交通灯【】、智能操盘交易【】、图像编辑【】、人类姿态评估【】、事件摘要【】、句子配对【】、基于文本的游戏【】、图像处理【】、对话生成【】、基于DAG图的资源分配【】、自然语言生成【】、恶意软件检测【】、政府决策【】

**1.Dynamics-Aware Unsupervised Skill Discovery**
- 链接 : <https://openreview.net/pdf?id=HJgLZR4KvH>
- 作者 : Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman
- 单位 : Google Brain
- 摘要：传统上，基于模型的强化学习（MBRL）旨在学习环境动力学的全局模型。一个好的模型可以潜在地使计划算法生成各种各样的行为并解决各种任务。但是，要为复杂的动力学系统学习准确的模型仍然很困难，即使如此，该模型也可能无法在训练它的状态分布之外很好地推广。在这项工作中，我们结合了基于模型和无模型的学习方法，使基于模型的计划变得容易。为此，我们旨在回答这个问题：我们如何发现其结果易于预测的技能？我们提出了一种无监督的学习算法，即“动态感知技能发现（DADS）”，该算法同时发现可预测的行为并了解其动态。我们的方法可以利用连续的技能空间，理论上使我们能够学习无数种行为甚至对于连续状态空间。我们证明，在学习潜在空间中的零样本规划显着优于标准MBRL和无模型的目标条件RL，可以处理稀疏任务，并且与以前的分层RL方法相比有显着改进对于无监督的技能发现
- 创新模式：老问题首次引入某种新技术或提出新方法或改进
- 技术总结：有模型，单agent，层次的 
- 关键词：高效探索采样，逐步分层解决，无监督强化学习（新增），稀疏奖励（新增）
- 源代码：<https://github.com/google-research/dads>
- 贡献：我们工作的关键贡献是建立在基于互信息的探索基础上的无监督强化学习算法DADS。 我们证明了我们的目标可以在连续的空间中嵌入学习过的原语，这使我们能够学习大量多样的技能。 至关重要的是，我们的算法还学会了对技能动态进行建模，从而可以将基于模型的计划算法用于下游任务。 我们采用了传统的模型预测控制算法来在原语空间中进行计划，并证明我们可以组合学习到的原语来解决下游任务，而无需任何额外的训练。

**2.Contrastive Learning of Structured World Models**
- 链接 | https://openreview.net/pdf?id=H1gax6VtDB
- 作者 | Thomas Kipf, Elise van der Pol, Max Welling
- 单位 | University of Amsterdam
- 负责 | 沈硕

**3.Implementation Matters in Deep RL: A Case Study on PPO and TRPO**
- 链接 | https://openreview.net/pdf?id=r1etN1rtPB
- 作者 | Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
- 负责 | 穆亭江

**4.GenDICE: Generalized Offline Estimation of Stationary Values**
- 链接 | https://openreview.net/pdf?id=HkxlcnVFwB
- 作者 | Ruiyi Zhang, Bo Dai, Lihong Li, Dale Schuurmans
- 单位 | Duke University; Google Brain
- 负责 | 蒋檄文


**5.Causal Discovery with Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1g2skStPB
- 作者 | Shengyu Zhu, Ignavier Ng, Zhitang Chen
- Huawei Noah’s Ark Lab; University of Toronto
- 负责 | 刘辰宇


**6.Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?**
- 链接 | https://openreview.net/pdf?id=r1genAVKPB
- 作者 | Simon S. Du, Sham M. Kakade, Ruosong Wang, Lin F. Yang
- 单位 | University of Washington; Carnegie Mellon University; University of California, Los Angles
- 负责 | 刘辰宇

**7.Harnessing Structures for Value-Based Planning and Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rklHqRVKvH
- 作者 | Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi
- 单位 | MIT
- 负责 | 刘辰宇

**8.Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency**
- 链接 | https://openreview.net/pdf?id=SJgzLkBKPB
- 作者 | Piyush Gupta, Nikaash Puri, Sukriti Verma, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, Sameer Singh
- 单位 | Adobe;
- 负责 | 郭智慧
- 摘要 | 随着深度强化学习应用到更多的任务上，我们需要去可视化和理解agents的行为。saliency maps通过突出和agent采取行动最相关的输入状态解释了agent的行为。现存的基于扰动的计算显著性的方法通常会突出显示与agent所采取的行动无关的输入区域。我们提出的方法，SARFA（特定的和相关的特征分布），通过平衡两个方面（特异性和相关性）捕捉不同的突出需求生成更集中的显著性地图。第一部分捕获了扰动对待解释的行为的相对期望回报的影响。第二部分降低了一些无关的特征的权重，这些特征改变了行为的相对预期回报，而不是需要解释的行为。我们对比了SARFA和现存的方法在玩儿棋类游戏和Atari游戏上的不同。我们通过说明性的例子(国际象棋，雅达利，围棋)，人类研究(国际象棋)，和自动化评估方法(象棋)，SARFA生成的显著性地图比现有的方法对人类更具有可解释性。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 技术和强化学习没什么关系
- 关键词 | 强化学习可解释性
- 源代码 | https://nikaashpuri.github.io/sarfa-saliency/
- 贡献 | SARFA来解释agent在棋盘游戏(国际象棋和围棋)中所采取的行动和雅达利游戏(Breakout)。Pong和太空入侵者)。使用一些说明性的例子。我们表明与现有方法相比，SAREA对所有这些设置获得了更集中、更准确的解释。我们还证明SARFA在识别国际象棋谜题中的重要棋子方面更有效，更进一步，在帮助熟练的国际象棋玩家解决国际象棋谜题方面(与现有方法相比，SARFA提高了近25%的解决精度，减少了31%的时间)。


**9.Meta-Q-Learning**
- 链接 | https://openreview.net/pdf?id=SJeD3CEFPH
- 作者 | Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J. Smola
- 单位 | Amazon; University of Pennsylvania
- 负责 | 胡小波
- 摘要 | 本文介绍了Meta-Q-Learning（MQL），这是一种用于元强化学习（meta-RL）的新的非策略算法。 MQL基于三个简单的想法。首先，作者证明了当给定一个上下文变量来表示过去的轨迹时，Q学习与最先进的meta-RL算法相比是具有竞争力的。 第二，多任务目标下训练任务中最大化平均奖励的是对RL策略进行元训练的有效方法。 第三，元训练回放缓冲区中的过去数据可以被回收，从而便于使用非策略更新算法来调整新任务的策略。MQL利用倾向于估计的思想进行实现，从而扩大了可用于适应的数据量。在标准连续控制基准测试上的实验表明，MQL与meta-rl中的最新技术相比有优势。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 元强化学习，有模型，单智能体
- 关键词 | 元强化学习，稀疏奖励，机器人（新增）
- 源代码 | 暂未公开
- 贡献 | 本文提出的算法，即MQL，基于三个简单的思想。首先，具有上下文的Q学习方法足以在当前的meta-RL基准中具有竞争力。第二，最大化训练任务的平均报酬是一种有效的元学习技术。 虽然MQL的元训练阶段比现有算法要简单得多，但是它可以达到与现有算法相媲美的性能。这表明我们需要在如深层网络这样丰富的函数逼近器的上下文背景下重新考虑元学习。第三，如果要用很少的数据来适应新任务，则必须利用每种可用的途径。 MQL使用倾向估计技术从元训练重播缓冲区中回收数据。这些数据基本上是免费的，并且被其他算法完全忽略。这个想法可以潜在地用于RL之外的问题，例如少样本和零样本分类。最后，本文阐明了meta-R中基准环境的本质。事实上，即使是没有元训练和任何调整的上下文变量的普通Q-学习也与最先进的算法相竞争，这表明（i）当前meta-RL基准中的训练和验证任务彼此非常相似，（ii）当前的基准可能不足以评估meta-RL算法。这两者都是行动的号召，并指出需要投入资源为metarl创造更好的基准问题，从而推动新算法的创新。

**10.Discriminative Particle Filter Reinforcement Learning for Complex Partial observations**
- 链接 | https://openreview.net/pdf?id=HJl8_eHYvS
- 作者 | Xiao Ma, Peter Karkus, David Hsu, Wee Sun Lee, Nan Ye
- 单位 | National Unviersity of Singapore; The University of Queesland
- 负责 | 蒋檄文

**11.Disagreement-Regularized Imitation Learning**
- 链接 | https://openreview.net/pdf?id=rkgbYyHtwB
- 作者 | Kiante Brantley, Wen Sun, Mikael Henaff
- 单位 | University of Maryland; Microsoft Research
- 负责 | 郭智慧
- 摘要 | 我们提出了一个简单而有效的算法来解决模仿学习中的协变量转移问题。它通过对专家演示数据的策略集合进行培训，并使用他们预测的方差作为成本，而RL和监督行为克隆成本使其最小化。与对抗的模仿方法不同，它使用固定的优化很简单的奖励函数。我们证明了算法的遗憾界，即在时间范围内是线性的，再乘以一个系数，对于某些行为克隆失败的问题，这个系数是低的。我们在基于多个像素的Atari环境和连续控制任务中对我们的算法进行了经验评估，结果显示，它匹配或显著优于行为克隆和生成性对抗模仿学习。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 模仿学习
- 关键词 | 监督和强化混合模型 模仿强化学习
- 源代码 | 没找到
- 贡献 | 解决了协变量转变的问题

**12.Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation**
- 链接 | https://openreview.net/pdf?id=S1glGANtDr
- 作者 | Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, Qiang Liu
- 单位 | The University of Texas at Austin; Google Research
- 负责 | 蒋檄文

**13.SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference**
- 链接 | https://openreview.net/pdf?id=rkgvXlrKwH
- 作者 | Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, Marcin Michalski
- 单位 | Google Research
- 负责 | 韩帅

**14.The Ingredients of Real World Robotic Reinforcement Learning**
- 链接 | <https://openreview.net/pdf?id=rJe2syrtvS>
- 作者 | Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, Sergey Levine
- 负责 | 赵亮
- 摘要：在许多情况下，针对现实世界机器人技术的强化学习的成功仅限于仪器化实验室场景，通常需要人员辛苦的参与和监督才能实现连续学习。在本文的工作中，我们讨论了机器人学习系统所需的要素，该系统可以随着在现实世界中收集的数据而不断地自主地进行改进。我们使用灵巧操作（dexterous manipulation）作为案例研究，提出了这样一个系统的特定实例。 随后，我们研究了在没有仪器的情况下学习时会遇到的许多挑战。在这样的设定下：没有人工设置的重置；仅使用机器人自身传感器（on-board perception）；并且无需人为设计的奖励；，学习也必须是可行的。我们提出了简单且易扩展的解决方案来解决这些挑战，并在一组灵巧机器人任务上证明了我们我们提出的系统的效果，从而提出了与这一学习范例有关的挑战的深入分析。我们证明了我们完整的系统可以在没有任何人工干预的情况下进行学习，使用真实的三指手（three-fingered hand）可以掌握各种基于视觉的技能。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。
- 技术总结：无模型，单agent，不确定性 
- 关键词：无监督学习，机器人(新增)，VAE（新增），随机扰动控制器
- 源代码：<https://github.com/NagarajSMurthy/Top-RL-papers-from-ICLR-2020>
- 贡献：这项工作的主要贡献是为现实世界机器人的连续无监督学习提出了一个系统，并提出了这种系统的实际实例。 整个方法使用soft-actor-crtic通过视觉观察进行学习，并使用VICE进行基于分类器的奖励，引入辅助重建目标或预训练编码器以进行无监督表示学习，并在训练过程中使用扰动控制器来增加更多的初始状态的探索。作者将这个系统命名为R3L。


**15.Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search**
- 链接 | https://openreview.net/pdf?id=BJlQtJSKDB
- 作者 | Anji Liu, Jianshu Chen, Mingze Yu, Yu Zhai, Xuewen Zhou, Ji Liu
- 单位 | Tencent AI Lab
- 负责 | 林晨
- 摘要：蒙特卡罗树搜索(MCTS)算法在许多具有挑战性的基准测试(如计算机围棋)中取得了巨大的成功。但是，它们通常需要大量的发布，这使得它们的应用程序成本很高。此外，由于MCTS固有的顺序特性，并行化MCTS也极具挑战性:每次推出都严重依赖于从以前的模拟中估计的统计数据(例如，节点访问计数)，以实现有效的探索-利用权衡。尽管存在这些困难，我们开发了WU-UCT1算法来有效地并行化mct，该算法实现了线性加速，并且随着worker数量的增加，其性能损失有限。WU-UCT的关键思想是一组统计信息，我们引入它来跟踪正在进行但不完整的模拟查询(称为未观察样本)的数量。当我们并行化最耗时的扩展和模拟步骤时，这些统计数据被用来在有原则的方式下修改UCT树策略，以保持有效的探索-开发权衡。在一个专有的基准测试(快手「点消快乐城」游戏)和Atari游戏基准测试上的实验表明，与现有技术相比，WU-UCT的线性加速和优越性能。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。
- 技术总结：有模型，单agent，不确定性 
- 关键词：蒙特卡洛搜索树（UCT），并行化, 
- 源代码：<https://github.com/liuanji/WU-UCT>
- 贡献：本文主要贡献如下:本文提出了WU-UCT并行MCTS算法，该算法通过观察未观察样本的数量来解决并行过程中统计量过时的问题。在新设计的统计数据的基础上，对UCT节点选择策略进行了原则性的修改，实现了探索性与开发性的有效权衡。以效率为导向的系统实现一起，WU-UCT实现了接近最优的线性加速，只有有限的性能损失，跨越广泛的任务，包括快手「点消快乐城」游戏和雅达利游戏。

**16.Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization**
- 链接 | https://openreview.net/pdf?id=ryeYpJSKwr
- 作者 | Michael Volpp, Lukas P. Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, Christian Daniel
- 负责 | 林晨

**17.A Closer Look at Deep Policy Gradients**
- 链接 | https://openreview.net/pdf?id=ryxdEkHtPS
- 作者 | Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
- 单位 | MIT，Two Sigma
- 负责 | 胡小波
- 摘要 | 我们研究深度策略梯度算法的行为如何反映激励其发展的概念框架。 为此，我们基于该框架的关键元素（梯度估计，值预测和优化环境）提出了对最新方法的细粒度分析。 我们的结果表明，深层策略梯度算法的行为通常会偏离其激励框架的预测：代理目标与真实奖励格局不匹配，学习的价值估算器无法拟合真实价值函数，并且梯度估算与“真实”梯度的相关性差。 我们发现的预测行为和经验行为之间的不匹配突出了我们对当前方法的理解不足，并表明有必要超越当前以基准为中心的评估方法，转换为对深度RL的细致理解。
- 创新模式 | 针对老问题给出理论研究结果
- 技术总结 | 单智能体，有模型
- 关键词 | 算法理论研究、 稳定性鲁棒性不确定性
- 源代码 | 暂未公开
- 贡献 | 在这项工作中，我们分析了深度策略梯度算法的关键原语遵循其概念基础的程度。 我们的实验表明，这些基元通常不符合预期的行为：梯度估计与真实梯度的相关性较差；更好的梯度估计可能需要较低的学习率，并且可能导致智能体行为退化；值网络将梯度估计方差降低到比真值小得多的程度价值，并且潜在的优化前景可能会产生误导。 这表明启发当前算法的理论与驱动其性能的实际机制之间存在巨大差距。 总体而言，我们的发现表明，开发真正强大且可靠的深度RL工具包将需要从当前的基准驱动评估模型转变为对深度RL算法的更细致的理解。

**18.Fast Task Inference with Variational Intrinsic Successor Features**
- 链接 | https://openreview.net/pdf?id=BJeAHkrYDS
- 作者 | Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, Volodymyr Mnih
- 单位 | DeepMind
- 负责 | 蒋檄文

**19.Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees**
- 链接 | https://openreview.net/pdf?id=rJgJDAVKvB
- 作者 | Binghong Chen, Bo Dai, Qinjie Lin, Guo Ye, Han Liu, Le Song
- 单位 | Georgia Institute of Technology; Google Research; Northwestern University
- 负责 | 刘辰宇

**20.Dream to Control: Learning Behaviors by Latent Imagination**
- 链接: https://openreview.net/pdf?id=S1lOTC4tDS
- 作者: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi
- 单位: University of Toronto; DeepMind; Google Brain
- 摘要: 学习一个总结了智能体经验的世界模型有助于学习复杂的行为。深度学习使从高维的输入中学习环境模型成为可能，同时也有很多潜在的方法可以从环境模型中学习行为。我们提出了Dreamer，一个强化学习智能体从图像输入中通过隐空间想象解决长视野任务。我们通过高效的从学到的环境模型中想象出来的轨迹序列中对价值函数进行梯度传播来学习策略。在20个图像输入的任务中，Dreamer在样本效率、计算时间、最终结果上都超过了现有的方法。
- 创新模式: 老问题首次引入某种新技术或提出新方法或改进，基于作者
- 技术总结: 基于模型的（world model）, 单智能体 ,确定性环境（mujoco）
- 关键词: 状态隐空间（latent dynamics）中控制， 多步reward（n-step TD）,Actor-Critic
- 源代码以及其他资料: https://danijar.com/project/dreamer/
- 贡献: 通过在状态隐空间上想象，相比其他model base agent拥有更长的视野（long-horizon）。在DeepMind Control Suite 的20个任务上运行，在样本效率，计算时间、最终表现上都超过了现有的方法

**21.Making Efficient Use of Demonstrations to Solve Hard Exploration Problems**
- 链接: https://openreview.net/pdf?id=SygKyeHKDH
- 作者: Caglar Gulcehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team
- 单位: DeepMind
- 摘要: 本文介绍了R2D3算法( RECURRENT REPLAY DISTRIBUTED DQN FROM DEMONSTRATIONS)，训练出的智能体可以有效利用演示样本来解决在<u>初始条件高度可变</u>的<u>部分可观察环境</u>中的<u>探索困难</u>的问题。我们还引入了包含这三个属性的八个任务z组件，并证明了R2D3可以解决其他一些先进方法（无论有无演示）在几百亿个探索轨迹后仍不能获得一次成功的轨迹的任务。
- 创新模式: 老问题首次引入某种新技术或提出新方法或改进
- 技术总结: 无模型，单智能体
- 关键词: 演示样本，探索，循环网络(rnn)，部分可观，环境初始高度可变
- 源代码: 无
- 贡献:在R2D2算法的基础上引进domo-ration因子，融合演示样本与agent探索经验，能够有效提取出演示中的有效信息，在其基础上提高探索效果。

**22.Intrinsic Motivation for Encouraging Synergistic Behavior**
- 链接 | <https://openreview.net/pdf?id=SJleNCNtDH>
- 作者 | Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, Abhinav Gupta
- 单位 | MIT; Facebook AI Research
- 负责 | 赵亮
- 摘要：我们研究内在动机作为探索偏见在稀疏奖励协同任务的强化学习中所起的作用。这些任务是单个Agent无法单独实现，需要多个Agent共同努力才能完成。我们关键的想法是，协同任务内在动机的一个好的指导方针，采取行动去影响世界，而不是影响自己。因此，我们提出鼓励Agent采取联合行动。基于这样的想法，我们研究了两种实例：一种基于遇到的真实状态，另一种基于与策略同时训练的动力学模型。尽管前者比较简单，但后者的好处是所采取的措施在分析上可以区分。我们在使用稀疏奖励的双手操控机器人和多Agent移动任务中验证了我们的方法。我们研究发现，我们的方法效率高于：1）仅使用稀疏奖励进行训练2）使用内在动机的典型基于惊喜的方法（surprise-based）。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。
- 技术总结：多智能体
- 关键词：多智能体合作，intrinsic motivation
- 源代码：暂无
- 贡献：在这项工作中，我们提出了一种内在动机的表述，这种内在动机可以鼓励协同行为，并可以有效地学习稀疏奖励任务，例如双向操纵和多主体移动。与非协同式的内在动机相比，效果显著。

**23.SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards**
- 链接 | https://openreview.net/pdf?id=S1xKd24twB
- 作者 | Siddharth Reddy, Anca D. Dragan, Sergey Levine
- 单位 | UC Berkeley
- 负责 | 刘辰宇

**24.Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives**
- 链接 | https://openreview.net/pdf?id=ryxgJTEYDr
- 作者 | Anirudh Goyal, Shagun Sodhani, Jonathan Binas, Xue Bin Peng, Sergey Levine, Yoshua Bengio
- 负责 | 赵亮
- 摘要：在各种复杂环境中运行的强化学习Agent可以从其行为的结构分解中受益。通常，这是在分层强化学习的上下文中解决的，其目的是将策略分解为较低级别的原语或选项，以及将较高级别的元策略分解为针对给定情况触发适当行为的策略。但是，元政策(meta-policy)仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原始内容，类似于层次强化学习，但没有明确的高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用一种信息理论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息来做出决定，而原语会选择所需有关当前状态的最多信息的行为在环境中执行。规范原语以使用尽可能少的信息会导致自然竞争和专业化。 我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。
- 技术总结：元强化学习
- 关键词：元策略，原语
- 源代码：<https://github.com/maximecb/gym-minigrid>
- 贡献：本文主要贡献如下：（1）我们提出了一种以分散方式学习和操作一组功能原语的方法，而无需明确的高级元控制器来选择活动原语。（2）我们引入了一个信息理论目标，其效果是双重的：1）导致单个原语专门化到状态空间的不同区域，2）启用竞争机制，该竞争机制用于选择 分散的活动原语。 （3）我们证明了我们模型的卓越转移学习性能，这是由于所提出的框架在原语的动态添加，删除和重组方面具有灵活性。 分散的基元可以成功地转移到较大的或以前看不见的环境中，并且在使用显式元控制器进行基元选择方面表现优于模型。

**25.Multi-Agent Interactions Modeling with Correlated Policies**
- 链接 | https://openreview.net/pdf?id=B1gZV1HYvS
- 作者 | Minghuan Liu, Ming Zhou, Weinan Zhang, Yuzheng Zhuang, Jun Wang, Wulong Liu, Yong Yu
- 单位 | Shanghai Jiaotong University; Huawei Noah’s Ark Lab
- 负责 | 许鑫悦

**26.Influence-Based Multi-Agent Exploration**
- 链接 | https://openreview.net/pdf?id=BJgy96EYvr
- 作者 | Tonghan Wang, Jianhao Wang, Yi Wu, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 胡小波
- 摘要 | 激励强化学习的本质旨在解决稀疏奖励任务的探索挑战。但是，目前文献中基本没有研究依赖于过渡的多主体环境的探索方法。我们目标是朝着解决这个问题迈出一步。我们介绍了两种探索方法：利用信息在理论协同影响中的作用，通过信息理论影响（EITI）进行探索，以及通过决策理论影响（EDTI）进行探索。 EITI使用相互信息来捕获代理程序过渡动态之间的相互依存关系。 EDTI使用一种称为“互动价值（VoI）”的内在奖励来表征和量化一个智能体的行为对其他智能体预期回报的影响。通过优化EITI或EDTI目标作为规范化工具，鼓励智能体协调其探索和学习策略从而优化团队绩效。我们展示了如何优化这些调节器，以便它们可以轻松地与策略梯度强化学习集成。最终的更新规则将协调的探索与内在的奖励分配联系起来。最后，我们实验证明了我们的方法在多种多智能体场景中的强大优势。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 多agent，有模型，通信理论
- 关键词 | 稀疏奖励、高效探索采样、 多智能体合作
- 源代码 | 暂未公开
- 贡献 | 在本文中，我们研究了多主体探索问题，并提出了两种基于影响的方法来开发交互结构。 这些方法基于两种交互作用量度，即MI和VoI，它们分别衡量一个智能体对另一智能体的勘探过程的影响程度和价值。 最好将这两种措施重新分配为探索奖励分配。 我们还提出了一种策略梯度框架中的优化方法，该方法使代理能够以分散的方式实现协同探索并优化团队绩效。

**27.Learning the Arrow of Time for Problems in Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rylJkpEtwS
- 作者 | Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, Yoshua Bengio
- 单位 | MILA
- 负责 | 穆亭江

**28.AMRL: Aggregated Memory For Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=Bkl7bREtDr
- 作者 | Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, Katja Hofmann
- 单位 | Microsoft Research
- 负责 | 林晨

**29.Model Based Reinforcement Learning for Atari**
- 链接：https://openreview.net/pdf?id=S1xCPJHtDB
- 作者：Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski
— 单位：Google Brain
- 摘要：Model Free强化学习可以在复杂的环境下学习有效的策略，即使是Atari游戏中以图像作为输入。然而这通常需要大量的环境交互，通常比人类学习同样的游戏所需的多的多。为什么人类可以学的这么快？部分原因可能是人类能够学习游戏是如何运作的，然后预测某个动作可能产生的后果。这篇文章中我们探索视频图像预测模型如何让agent使用比model free方法更少的样本解决Atari游戏问题。我们提出SimPLe，一种基于视频图像输入的完全Model Base方法，同时提出几种Model 网络结构。我们在一系列Atari游戏中评估了SimPLe，同时环境交互数量限制在100K这个较低的数量上，等同于在游戏中运行两小时。在大部分游戏中表现都超过了Model Free方法，在部分游戏中效果超过了一个量级。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进
- 技术总结：有模型的，单agent
- 关键词：Atari，图像输入，Model Base, Sample Efficiency
- 源代码：https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/rl
- 贡献：SimPLe通过图像输入学到了高效的策略来玩Atari游戏，在大部分的游戏中只需不到100K样本交互，2小时的游戏时间。Model的隐变量部分能有效处理随机性。

**30.Variational Recurrent Models for Solving Partially Observable Control Tasks**
- 链接 | https://openreview.net/pdf?id=r1lL4a4tDB
- 作者 | Dongqi Han, Kenji Doya, Jun Tani
- 负责 | 宋明惠

**31.Sample Efficient Policy Gradient Methods with Recursive Variance Reduction**
- 链接 | https://openreview.net/pdf?id=HJlxIJBFDr
- 作者 | Pan Xu, Felicia Gao, Quanquan Gu
- 单位 | University of California, Los Angeles
- 负责 | 王善锐
- 摘要：如何提高强化学习的采样效率，长久以来一直是一个研究难题。在本篇工作中，我们旨在减少现有策略梯度方法的采样复杂性。我们提出了一个新的策略梯度算法，叫SRVR-PG，该算法只需要$O(1/\epsilon^(3/2))$个episodes，来找到非凹性能方程$J(\theta)$(nonconcave performance function)的$\epsilon$-近似稳定点($\epsilon$-approximate stationary point)，比如得到参数$\theta$令$||\nabla J(\theta)||^2_2 \leq \epsilon$。该采样复杂度可提高现有随机方差减小算法的结果$O(1/\epsilon^(5/3))$，再缩短至$O(1/\epsilon^(1/6))$分之一。另外，我们也在SRVR-PG的基础上做进一步改进，添加了参数探索机制，即从一个先验的概率分布中探索初始策略参数。我们在一些RL的经典控制问题中，进行了多个实验，以此证明我们提出算法的性能。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进，对传统PG算法做改进，提高PG的采样效率
- 技术总结：无模型，单agent
- 关键词：策略梯度，采样复杂度，策略梯度估计器，基于参数的探索
- 源代码：无
- 贡献：我们提出了一个新的策略梯度方法，SRVR-PG，该方法会循环更新策略梯度估计器。我们证明了SRVR-PG的采样复杂度要低于SVRPG算法。我们同时也提出了一个新的减少策略梯度的方差的技术，通过引入基于参数的探索，并提出了SRVR-PG-PE，该算法在理论和实践中的表现都要优于PGPE算法。

**32.Exploring Model-based Planning with Policy Networks**
- 链接 | https://openreview.net/pdf?id=H1exf64KwH
- 作者 | Tingwu Wang, Jimmy Ba
- 单位 | University of Toronto; Vector Institute
- 负责 | 沈硕

**33.Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation**
- 链接 | https://openreview.net/pdf?id=HygnDhEtvr
- 作者 | Yu Chen, Lingfei Wu, Mohammed J. Zaki
- 单位 | Rensselaer Polytechnic Institute; IBM Research
- 负责 | 穆亭江

**34.RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments**
- 链接 | https://openreview.net/pdf?id=rkg-TJBFPB
- 作者 | Roberta Raileanu, Tim Rocktäschel
- 单位 | New York University; University College London
- 负责 | 宋明惠

**35.Learning Expensive Coordination: An Event-Based Deep RL Approach**
- 链接 | https://openreview.net/pdf?id=ryeG924twB
- 作者 | Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, Bo An
- 单位 | Nanyang Technological University; Sun Yat-sen University
- 负责 | 王善锐
- 摘要：当前关于多智能体强化学习的工作大都关注完全合作的智能体、联合地完成特定任务。但是，在现实世界的许多案例中，智能体往往更关注于自身，比如公司中的雇员或联盟中的俱乐部。因此领导者(leaders)，比如公司或者联盟的管理者(managers)，需要为那些有效协作(efficient coordination)的追随者(followers)提供额外的奖励(bonuses)，我们将此称为昂贵协作(expensive coordination)。昂贵协作的难点主要在于：(1)领导者在分配奖励时，必须考虑长期的影响，以及预测追随者未来的行为；(2)追随者之间复杂的交互让训练过程难以收敛，尤其当领导者的策略也在随时间变化。在本篇工作中，我们解决这些问题主要通过基于事件(event-based)的DRL方法。我们的贡献主要有三个方面：(1)我们将领导者的决策过程建模成一个半马尔可夫过程(semi-MDP)，并提出了一个全新的基于事件的多智能体策略梯度，用来学习领导者的长期策略；(2)我们利用了领导者-追随者一致性(leader-follower consistency)模式来设计追随者感知(follower-aware)模块，和特定追随者注意力模块(follower-specific attention module)，用于预测追随者的行为，并对他们的行为做出准确回应；(3)我们提出了基于动作抽象的策略梯度算法来减少追随者决策空间，进而加速了追随者的训练速度。在资源收集、移动、捕食者与猎物游戏环境中，证明了我们的方法要优于其他的一些方法。
- 创新模式：将某个已经使用了RL方法的应用或理论问题进行问题场景细分，多智能体问题环境下，针对之前的全合作任务环境进行进一步划分，智能体更具有自主性，需要一个leader来统筹协调，在该问题下进一步提出方法。
- 技术总结：多agent，层次的，无模型
- 关键词：多智能体，领导者-追随者，基于事件RL
- 源代码：无
- 贡献：1)考虑到领导者的行为的长期的影响，我们提出了基于事件的策略梯度，用于领导者的策略的更新；2)为了预测追随者的行为，并对他们的行为做出正确的回应，我们利用了领导者-追随者一致性，来设计一个全新的追随者感知模块和特定追随者注意力机制；3)我们提出了基于动作抽象的策略梯度算法来加速追随者的训练。

**36.Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning**
- 链接 | <https://openreview.net/pdf?id=SJxbHkrKDH>
- 作者 | Qian Long, Zihan Zhou, Abhinav Gupta, Fei Fang, Yi Wu, Xiaolong Wang
- 单位 | CMU; OpenAI; Facebook AI Research; SJTU; UCSD
- 负责 | 赵亮
- 摘要：在多智能体游戏中，环境的复杂性会随着智能体数量的增加而呈指数级增长，所以，当智能体的群很大时，如何学习出好的策略是非常具有挑战的。在本文中，作者提出了进化种群训练法（Evolutionary Population Curriculum，EPC），这是一种课程学习范例，它通过逐步增加智能体群的数目来扩大多智能体强化学习（MARL）的规模。此外，EPC使用进化的方法来解决整个训练过程中客观存在的不匹配问题：在早期以少量智能体成功训练的智能体不一定是适应后期智能体种群规模扩大的情况。具体而言，EPC在每个阶段维护多组智能体，对这些组执行混合匹配和微调，并以最佳适应性促进这些智能体对下一阶段训练的推广。我们在一种流行的MARL算法MADDPG上实现了EPC，并通过经验证明，随着代理数量呈指数增长，我们的方法始终在性能上始终优于基线。EPC与具体的RL算法无关，它可以与大多数现有的MARL算法集成。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。提出一种处理多智能体种群数量多的解决办法，即使用进化的方式，从小规模到大规模。
- 技术总结：多智能体，无模型，大规模
- 关键词：多智能体，进化训练，大规模
- 源代码：<https://github.com/qian18long/epciclr2020>
- 贡献：1、我们提出了通过逐步增加智能体群的数目来扩大多智能体强化学习（MARL）的规模的方法EPC。2、我们的方法不仅在性能上而且在训练稳定性上都表现出比基线有显着改善。3、我们认为我们的方法是通用的，并且可能有益于扩展其他MARL算法。

**37.Making Sense of Reinforcement Learning and Probabilistic Inference**
- 链接 | https://openreview.net/pdf?id=S1xitgHtvS
- 作者 | Brendan O’Donoghue, Ian Osband, Catalin Ionescu
- 负责 | 刘辰宇

**38.Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs**
- 链接 | https://openreview.net/pdf?id=rkxDoJBYPB
- 作者 | Aditya Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, Oriol Vinyals
- 单位 | Google Research; DeepMind;
- 负责 | 赵亮
- 摘要：我们提出了一种深度强化学习方法，来最小化神经网络计算图在编译器中的执行成本。与早期的基于学习的方式需要在同一张图上对优化器进行训练以完成优化不同，我们提出了一种学习方法，该方法离线训练优化器，然后将其推广到未知的图，而无需进一步培训。这使得我们的方法可以在几秒钟而不是几小时内在现实世界的TensorFlow图上产生高质量的执行决策。我们为图计算考虑了两个优化任务：最小化运行时间和峰值内存使用率。与广泛的基准相比，在这两个任务上，我们的方法比经典方法和其他基于学习的方法效果要好。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进。
- 技术总结：单智能体，迁移适应能力
- 关键词：优化器，图计算，DRL
- 源代码：暂无
- 贡献：使用DRL的方式训练优化器，减少图计算的执行开销，并可以推广到未知图。

**39.Never Give Up: Learning Directed Exploration Strategies**
- 链接 | https://openreview.net/pdf?id=Sye57xStvB
- 作者 | Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, Charles Blundell
- 单位 | DeepMind
- 负责 | 王善锐

**40.Robust Reinforcement Learning for Continuous Control with Model Misspecification**
- 链接 | https://openreview.net/pdf?id=HJgC60EtwB
- 作者 | Daniel J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, Martin Riedmiller
- 单位 | DeepMind
- 负责 | 沈硕

**41.Synthesizing Programmatic Policies that Inductively Generalize**
- 链接 | https://openreview.net/pdf?id=S1l8oANFDH
- 作者 | Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, Armando Solar-Lezama
- 单位 | MIT; University of Pennsylvania
- 负责 | 宋明惠

**42.Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation**
- 链接 | https://openreview.net/pdf?id=r1lOgyrKDS
- 作者 | Xinjie Fan, Yizhe Zhang, Zhendong Wang, Mingyuan Zhou
- 单位 | University of Texas at Austin; Microsoft Research; Columbia University
- 负责 | 蒋檄文

**43.Improving Generalization in Meta Reinforcement Learning using Learned Objectives**
- 链接 | https://openreview.net/pdf?id=S1evHerYPr
- 作者 | Louis Kirsch, Sjoerd van Steenkiste, Juergen Schmidhuber
- 负责 | 郭智慧
- 摘要 | 生物进化已经把许多学习者的经验提炼成人类的一般学习算法。我们的新的元强化学习算法MetaGenRL就是受到了这个过程的启发。MetaGenRL从许多复杂的agent中提取经验，以元学习一个低复杂度的神经目标函数，它决定未来个体将如何学习。与最近的元- rl算法不同，MetaGenRL可以推广到与元训练完全不同的新环境中。在某些情况下，它甚至超过了人类设计的RL算法。MetaGenRL在元训练过程中使用了非策略二阶梯度，极大地提高了样本效率。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 元强化学习 迁移适应能力
- 关键词 | 元强化学习 高效探索采样 环境变化策略适应 
- 源代码 | http://louiskirsch.com/code/metagenrl
- 贡献 | 提出了一种新的元强化学习方法MetaGenRL，这种方法不会过拟合，并且明确地分离了策略和学习规则，能够使用元学习规则在完全不同的环境中对随机初始化的agent进行训练。MetaGenRL在元训练过程中使用了非策略二阶梯度，极大地提高了样本效率。与EPG相比，MetaGenRL具有更高的样本效率，并且在固定的环境交互预算下显著优于EPG。

**44.Single Episode Policy Transfer in Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rJeQoCNYDS
- 作者 | Jiachen Yang, Brenden Petersen, Hongyuan Zha, Daniel Faissol
- 单位 | Georgia Institute of Technology
- 负责 | 王翔森

**45.DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames**
- 链接 | https://openreview.net/pdf?id=H1gX8C4YPr
- 作者 | Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra
- 单位 | Georgia Institute of Technology; Facebook AI Research
- 负责 | 赵亮

**46.Geometric Insights into the Convergence of Nonlinear TD Learning**
- 链接 | https://openreview.net/pdf?id=SJezGp4YPr
- 作者 | David Brandfonbrener, Joan Bruna
- 单位 | New York University
- 负责 | 蒋檄文

**47.Dynamics-Aware Embeddings**
- 链接 | https://openreview.net/pdf?id=BJgZGeHFPH
- 作者 | William Whitney, Rajat Agarwal, Kyunghyun Cho, Abhinav Gupta
- 单位 | New York University; Carnegie Mellon University; Facebook AI Research
- 负责 |  穆亭江

**48.Reanalysis of Variance Reduced Temporal Difference Learning**
- 链接 | https://openreview.net/pdf?id=S1ly10EKDS
- 作者 | Tengyu Xu, Zhe Wang, Yi Zhou, Yingbin Liang
- 单位 | Ohio State University; University of Utah
- 负责 | 蒋檄文

**49.Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP**
- 链接 | https://openreview.net/pdf?id=BkglSTNFDB
- 作者 | Yuanhao Wang, Kefan Dong, Xiaoyu Chen, Liwei Wang
- 单位 | Tsinghua University; Peking University
- 负责 | 胡小波
- 摘要 | 强化学习中的一个基本问题是无模型算法是否具有样本效率。 Jin等人在2018提出了一种基于UCB探索策略的Q学习算法，并证明了它对于有限回合MDP具有几乎最佳的遗憾界。 在本文中，我们在不访问生成模型的情况下，将带有UCB探索奖励的Q学习应用于具有折扣奖励的无限水平MDP。 我们表明，算法探索的样本复杂度界限受公式1的限制。 这改善了先前为人所知通过延迟Q学习的方法实现的公式2的结果，并且使影响因素与下界相匹配。 
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 无模型，单智能体
- 关键词 | 高效探索采样， 采样复杂度
- 源代码 | 暂未公布
- 贡献 | 带有折扣奖励的无限水平MDP是一种新的设置，它比其他常见的设置（例如有限水平MDP）难得多。在这种情况下，以前Strehl等人通过延迟Q学习的方法，使无模型强化学习算法实现的最佳样本复杂度范围为公式2。 在本文中，我们提出了一种Q学习的变体，它结合了置信度上限，并表明它的样本复杂度约为公式1。 除依赖于1 /（1--γ）和对数因子外，这与最佳下限匹配

**50.Automated curriculum generation through setter-solver interactions**
- 链接 | https://openreview.net/pdf?id=H1e0Wp4KvH
- 作者 | Sebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, Timothy Lillicrap
- 单位 | DeepMind

**51.Optimistic Exploration even with a Pessimistic Initialisation**
- 链接 | https://openreview.net/pdf?id=r1xGP6VYwH
- 作者 | Tabish Rashid, Bei Peng, Wendelin Boehmer, Shimon Whiteson
- 单位 | University of Oxford
- 负责 | 王善锐

**52.Multi-agent Reinforcement Learning for Networked System Control**
- 链接 | https://openreview.net/pdf?id=Syx7A3NFvH
- 作者 | Tianshu Chu, Sandeep Chinchali, Sachin Katti
- 单位 | Stanford University
- 负责 | 胡小波
- 摘要 | 本文研究了网络系统控制中的多智能体强化学习（MARL）。 具体来说，每个代理都基于本地观察和来自邻居的消息来学习分散控制策略。 我们将这种网络化的MARL（NMARL）问题描述为一个时空马尔可夫决策过程，并引入空间折扣因子来稳定每个局部智能体的训练。 此外，我们提出了一种新的可微通信协议，称为NeurComm，从而减少NMARL中的信息丢失和非平稳性。通过在自适应交通信号控制和协同自适应巡航控制的实际NMARL场景下的实验，适当的空间折扣因子有效地增强了非通信MARL算法的学习曲线，而NeurComm在学习效率和控制性能上也都优于现有的通信协议
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 有模型，多智能体，通信理论，合作
- 关键词 | 时空马尔可夫， 多智能体合作， 模拟仿真， 稳定性鲁棒性不确定性
- 源代码 | https://github.com/cts198859/deeprl_network.
- 贡献 | 本文的贡献有三个方面。首先，在NSC假设下，我们将NMARL描述为一个分散的时空MDP，并引入空间折扣因子来稳定训练，尤其是对于非通信算法。第二，我们提出了一种新的神经通讯协议NeurComm，它可以自适应地共享系统状态和智能体行为等信息。第三，我们设计并模拟现实的NMARL环境，从而评估和比较我们的方法与最先进的MARL基准算法。

**53.A Learning-based Iterative Method for Solving Vehicle Routing Problems**
- 链接 | https://openreview.net/pdf?id=BJe1334YDH
- 作者 | Hao Lu, Xingwen Zhang, Shuang Yang
- 单位 | Princeton University
- 负责 | 穆亭江

**54.Sharing Knowledge in Multi-Task Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rkgpv2VFvr
- 作者 | Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters
- 单位 | TU Darmstadt, IAS；Politecnico di Milano, DEIB
- 负责 | 胡小波
- 摘要 | 我们研究了在任务之间共享表示的好处，以便在多任务强化学习中有效使用深度神经网络。我们利用这样的假设：从不同的任务中学习，共享共同的属性，有助于推广它们的知识信息，与学习单个任务相比可以更有效地提取特征。直观地讲，当由强化学习算法使用时，所得的功能集可提供性能优势。我们通过提供理论上的保证来证明这一点，这些保证强调了在任务之间共享表示的条件，并将众所周知的“近似值迭代”的有限时间范围扩展到了多任务环境中。此外，我们通过提出三种强化学习算法的多任务扩展来补充我们的分析，这些算法是我们在广泛使用的强化学习基准上进行经验评估的结果，在样本效率和性能方面，它们比单任务同类算法有了显着改进。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 多agent， 确定性
- 关键词 | 通用游戏agent， 算法理论研究， 共享表示（新增）
- 源代码 | 暂未公开
- 贡献 | 我们从理论上证明了使用共享表示学习多个任务和学习单个任务的优势。我们利用了Maurer等人提供的MTL中近似误差的上界，已经得出了将AVI/API界限扩展到MTRL的结果，分析结果表明，随着任务数的增加，AVI/API迭代过程中的错误传播会减少。然后，我们提出了一种利用这一理论优势的实用方法，即通过深层神经网络有效地提取多个任务的共享表示。为了验证该方法的优越性，我们在提出的神经网络结构的基础上，引入了FQI、DQN和DDPG的多任务扩展，对具有挑战性的RL问题进行了实验研究。与预期的一样，良好的实证结果证实了我们所描述的理论效益

**55.RTFM: Generalising to New Environment Dynamics via Reading**
- 链接 | https://openreview.net/pdf?id=SJgob6NKvH
- 作者 | Victor Zhong, Tim Rocktäschel, Edward Grefenstette
- 单位 | University of Washington; University College London; Facebook AI Research
- 负责 | 穆亭江

**56.Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies**
- 链接 | https://openreview.net/pdf?id=HkgsWxrtPB
- 作者 | Sungryull Sohn, Hyunjae Woo, Jongwook Choi, Honglak Lee
- 单位 | University of Michigan; Google Brain
- 负责 | 王翔森

**57.Projection-Based Constrained Policy Optimization**
- 链接 | https://openreview.net/pdf?id=rke3TJrtPS
- 作者 | Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, Peter J. Ramadge
- 单位 | Princeton University;

**58.Graph Constrained Reinforcement Learning for Natural Language Action Spaces**
- 链接 | https://openreview.net/pdf?id=B1x6w0EtwH
- 作者 | Prithviraj Ammanabrolu, Matthew Hausknecht
- 单位 | Georgia Institute of Technology; Microsoft Researc
- 负责 | 宋明惠

**59.V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control**
- 链接 | https://openreview.net/pdf?id=SylOlp4FvH
- 作者 | H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin Riedmiller, Matthew M. Botvinick
- 单位 | DeepMind
- 负责 | 赵亮

**60.Thinking While Moving: Deep Reinforcement Learning with Concurrent Control**
- 链接 | https://openreview.net/pdf?id=Hke0V1rKPS
- 作者 | Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman, Alexander Herzog
- 单位 | Nanyang Technological University; MILA
- 负责 | 宋明惠

**61.Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rke7geHtwH
- 作者 | Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, Martin Riedmiller
- 单位 | DeepMind
- 负责 | 林晨

**62.Imitation Learning via Off-Policy Distribution Matching**
- 链接 | https://openreview.net/pdf?id=Hyg-JC4FDr
- 作者 | Ilya Kostrikov, Ofir Nachum, Jonathan Tompson
- 单位 | Google Research
- 负责 | 郭智慧

**63.Adversarial AutoAugment**
- 链接 | https://openreview.net/pdf?id=ByxdUySKvS
- 作者 | Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong
- 负责 | 韩帅

**64.Option Discovery using Deep Skill Chaining**
- 链接 | https://openreview.net/pdf?id=B1gqipNYwH
- 作者 | Akhil Bagaria, George Konidaris
- 单位 | Brown University
- 负责 | 王善锐
- 摘要：自主地发现时间上的拓展动作(temporally extended actions，也就是上层的输出)，或技巧(skill)，长久以来一直是分层强化学习的目标。我们提出了一种新的算法，该算法通过将技巧链接(skill chaining)与深度神经网络相结合，以此在高维、连续的问题域中，自动地发现技巧。最终得到的算法，deep skill chaining，构建skill，让其可以执行一个skill，进而让智能体去执行另一个（个人理解为skill之间是环环相扣的，因此叫skill chaining）。最终我们发现deep skill chaining在具有挑战性的连续控制任务中，不仅可以打败无分层结构的智能体，也优于其他基于skill discovery的分层强化学习算法。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进，将skill chaining和非线性函数近似相结合，引入到skill discovery的分层强化学习中去，用于解决高维的连续控制问题。
- 技术总结：无模型，单agent，层次的
- 关键词：skill chainig，分层强化学习，option框架
- 源代码：https://github.com/deep-skill-chaining/deep-skill-chaining
- 贡献：我们提出了新的skill discovery算法，DSC，用于解决高维的、目标导向的任务，并且表现明显由于普通的RL算法以及其他著名的分层方法。就我们所知，DSC是首个不把option的数量作为一个固定且代价高昂的超参数的深度option discovery算法；同时，其他深度option discovery算法都致力于展现其相较于不分层算法，在某个任务中的提高，而我们则展示了对任务分层的必要性。

**65.State-only Imitation with Transition Dynamics Mismatch**
- 链接 | https://openreview.net/pdf?id=HJgLLyrYwB
- 作者 | Tanmay Gangwani, Jian Peng
- 单位 | University of Illinois, Urbana-Champaign
- 负责 | 郭智慧

**66.The Gambler’s Problem and Beyond**
- 链接 | https://openreview.net/pdf?id=HyxnMyBKwB
- 作者 | Baoxiang Wang, Shuai Li, Jiajin Li, Siu On Chan
- 单位 | Chinese University of Hong Kong; Shanghai Jiao Tong University
- 负责 | 王善锐
- 摘要：我们分析了赌博机问题，一个简单的强化学习问题，在该问题中，赌博机有一定概率令其赌注加倍或者丢失，直到达到目标。该问题最早是作为一个简单的例子，在《Reinforcement Learning: An Introduction》一书中被介绍，其中提到了一种有趣的最优值函数模式，带有高频成分（high-frequency components）和重复非平滑点（repeating non-smooth points），但书中并没有给出进一步的解释。我们同时为离散动作问题和连续动作问题给出了最优值函数的具体形式。尽管它看起来比较简单，但值函数是无法控制的（pathlogical）：分形（fractal，分数维度）、自相似性（self-similar）、倒数取0或者无穷大（derivative taking either zero or infinity）、任何间隔都不平滑（not smooth on any interval）、无法写成初等函数的组合（not written as elementary functions）。事实上它是一种广义的康托函数，这种函数拥有迄今为止尚未发现的复杂性。我们的分析可以对提升值函数近似、策略梯度算法、Q-learning，在应用和实现中有更深刻的理解。
- 创新模式：针对老问题给出理论研究结果
- 技术总结：无模型，单agent，
- 关键词：赌博机问题，最优值函数，MDP
- 源代码：无
- 贡献：我们给出了不同设置下，赌博机问题的完整解。我们展示了最优值函数非常复杂，甚至是无法控制的，尽管看起来简单，但这些结果在之前的研究中都没有被指明。我们的贡献是理论上的发现和实现。其结果值得在社区中展开探讨。通过对赌博机问题指出来的，现在的大多数强化学习算法可能低估了复杂性。

**67.Structured Object-Aware Physics Prediction for Video Modeling and Planning**
- 链接 | https://openreview.net/pdf?id=B1e-kxSKDH
- 作者 | Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting
- 负责 | 沈硕

**68.Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery**
- 链接 | https://openreview.net/pdf?id=H1lmhaVtvr
- 作者 | Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine
- 负责 | 韩帅

**69.Exploration in Reinforcement Learning with Deep Covering Options**
- 链接 | https://openreview.net/pdf?id=SkeIyaVtwB
- 作者 | Yuu Jinnai, Jee Won Park, Marlos C. Machado, George Konidaris
- 单位 | Brown University; Google Brain
- 负责 | 胡小波

**70.CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1lEX04tPr
- 作者 | Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, Hongyuan Zha
- 单位 | Georgia Institute of Technology
- 负责 | 许鑫悦

**71.Learning to Coordinate Manipulation Skills via Skill Behavior Diversification**
- 链接 | https://openreview.net/pdf?id=ryxB2lBtvH
- 作者 | Youngwoon Lee, Jingyun Yang, Joseph J. Lim
- 单位 | University of Southern California
- 负责 | 王善锐
- 摘要：当掌握一项复杂的操作任务时，人类通常会将任务分解成身体对应部分的子技巧(sub-skill)，并单独练习这些技巧，然后将这些子技巧组合并应用。相似地，一个有多个末端执行器(end-effectors)的机器人，可以通过协调各个执行器的子技巧，进而完成复杂的任务。认识到时间上和行为上技巧的协调，我们提出了一个模块化框架，在该模块下首先通过技巧行为多样化(skill behavior diversification)来单独地训练每个末端执行器的技巧，然后学习如何协调不同的技巧行为。 我们描述我们提出的框架可以有效地协调自身的技巧，来完成复杂的协调控制任务，比如拾起一个长木条、推动容器时将方块放入其中、两个ant智能体同时推动一个箱子。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进，协同控制问题引入技巧协调的多智能体分层强化学习方法
- 技术总结：无模型，多agent，层次的
- 关键词：技巧行为多样性，协同控制问题，互信息，元策略
- 源代码：https://clvrai.com/coordination
- 贡献：在本篇文章中，我们提出了一个模块化框架，通过技巧协调(skill coordination)来解决带有多智能体子技巧组合的任务。我们使用了有互信息最大化的熵最大来训练不同行为的原始技巧。为了协调已经学好的原始技巧，元策略不仅会预测每个智能体/末端执行器执行的技巧，还会预测控制选中技巧的行为的行为embedding。

**72.Composing Task-Agnostic Policies with Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=H1ezFREtwH
- 作者 | Ahmed H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, Michael C. Yip
- 单位 | UC San Diego; University of Washington
- 负责 | 韩帅

**73.Frequency-based Search-control in Dyna**
- 链接 | https://openreview.net/pdf?id=B1gskyStwr
- 作者 | Yangchen Pan, Jincheng Mei, Amir-massoud Farahmand
- 单位 | University of Alberta; Vector Institute; University of Toronto
- 负责 | 沈硕

**74.Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=S1ltg1rFDS
- 作者 | Ali Mousavi, Lihong Li, Qiang Liu, Denny Zhou
- 单位 | Google Research; University of Texas, Austin
- 负责 | 韩帅

**75.CAQL: Continuous Action Q-Learning**
- 链接 | https://openreview.net/pdf?id=BkxXe0Etwr
- 作者 | Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, Craig Boutilier
- 单位 | Google Research
- 负责 | 王善锐
- 摘要：基于值函数的强化学习(Value-based Reinforcement Learning)方法，比如Q-learning，在很多领域中都取得了不错的成果。当把Q-learning用于解决连续动作空间中的问题时，有一个很大的挑战是连续动作最大化，需要最优贝尔曼回溯图(optimal Bellman backup)。在本篇工作中，我们提出了CAQL，一种用于连续动作Q学习的算法，可以使用多个即插即用优化器(plug-and-play optimizers)，用于解决max-Q问题。借用近期深度网络的优化结果，我们展示了max-Q问题可以通过混合整数规划(mixed-integer programming, MIP)来达到较优的解。虽然Q函数表示足够有效，基于MIP的优化可以得到更优的策略，并且比近似方法(比如梯度上升、交叉熵查找)有更好的鲁棒性。我们接着提出了几项用于加速CAQL的技术，尽管使用到了近似，但表现足够好。我们在多种带有动作约束的连续控制问题下，比较了CAQL和多种RL算法，发现CAQL在强约束环境下，表现要优于基于policy的算法。
- 创新模式：老问题首次引入某种新技术或提出新方法或改进，改进了基于值函数的RL算法在连续动作空间问题下的应用
- 技术总结：单agent，无模型
- 关键词：连续动作，Q值，混合整数规划，
- 源代码：无
- 贡献：(1)我们提出了CAQL框架，该框架通过选择一个即插即用动作优化器(“plug-and-play” action optimizers)，最小化Q-learning下的贝尔曼残差；(2)为了提高CAQL在大规模应用下的适用性，我们提出了3个加速技术:dynamic tolerance、dual filtering、clustering；(3)我们将CAQL与其他RL算法，在若干对动作进行不同程度约束的benchmark环境下进行比较，结果表明在强约束环境下，CAQL的表现要优于基于策略的RL算法。

**76.Reinforced active learning for image segmentation**
- 链接 | https://openreview.net/pdf?id=SkgC6TNFvr
- 作者 | Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J. Pal
- 单位 | MILA; Element AI
- 负责 | 郭智慧
- 摘要 | 基于学习的方法对于语义分割有两个固有的挑战。首先，获取像素级标签是费时费力的。第二，真实的分割数据集是高度不平衡的：一些类别远远比其他类别丰富的多，使得性能偏向于最多表现的种类。在本篇论文中，我们感兴趣的是将人类的标记工作集中在大数据池中的一个小子集上，使这种工作最小化，同时最大化一个保留集合上的分割模型的性能。我们提出了一种基于DRL的语义分割主动学习策略。一个agent学习一个策略从一个未标记的数据池中选择一个包含信息的小图片区域来进行标记，而不是整个图像区域。区域选择决策是根据所训练的分割模型的预测和不确定性做出的。我们的方法提出了一种主动学习的DQN新的改进，使它适应大规模的语义分割问题。我们在CamVid中测试概念证明，并在大规模Citycapes数据集中提供结果。在Cityscapes上，我们基于区域的DQN方法需要的额外标记数据比我们最具竞争力的baseline少大约30%，就能达到相同的性能。此外，我们发现，与baseline相比，我们的方法要求对代表性不足的类别使用更多的标签，从而提高它们的性能，并有助于减轻类别的不平衡。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 无模型 单agent 大规模 
- 关键词 | 大规模 主动学习 图像处理 DQN
- 源代码 | https://github.com/ArantxaCasanova/ralis
- 贡献 | 1、我们学习一个基于RL的函数来学习基于区域的主动学习图像分割。
         2、我们使用批处理模式的DQN来构建主动学习框架，它在每次主动学习迭代时并行标注多个区域(这是一种更有效的大规模数据集策略，与标准的迷你批处理梯度下降兼容)。
         3、我们测试了概念证明在CamVid数据集上，并且在Cityscapes数据集上提供了结果击败了最近最先进的BALD算法，一个被广泛应用的基于熵的选择准则和均匀采样基线。

**78.The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget**
- 链接 | https://openreview.net/pdf?id=Hye1kTVFDS
- 作者 | Anirudh Goyal, Yoshua Bengio, Matthew Botvinick, Sergey Levine
- 负责 | 王翔森

**79.Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation**
- 链接 | https://openreview.net/pdf?id=H1gzR2VKDH
- 作者 | Suraj Nair, Chelsea Finn
- 单位 | Stanford University; Google Brain
- 负责 | 宋明惠

**80.Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=BJliakStvH
- 作者 | Dexter R.R. Scobee, S. Shankar Sastry
- 单位 | UC Berkeley
- 负责 | 韩帅

**81.AutoQ: Automated Kernel-Wise Neural Network Quantization**
- 链接 | https://openreview.net/pdf?id=rygfnn4twS
- 作者 | Qian Lou, Feng Guo, Minje Kim, Lantao Liu, Lei Jiang.
- 负责 | 韩帅

**82.VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning**
- 链接 | https://openreview.net/pdf?id=Hkl9JlBYvr
- 作者 | Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson
- 单位 | University of Oxford; Microsoft Research
- 负责 | 林晨

**83.Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards**
- 链接 | https://openreview.net/pdf?id=SJg5J6NtDr
- 作者 | Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, Chelsea Finn
- 单位 | Google Brain; UC Berkeley; Stanford
- 负责 | 郭智慧
- 摘要 | 模仿学习允许agent从demo学习复杂的行为。然而，学习一个复杂的基于视觉的任务可能会需要不可估计的demos。元模仿学习是一种很有前途的方法，通过利用学习类似任务的经验，使agent能够从一个或几个demo中学习新任务。在任务模糊或未观察到的动态情况下，光有demo可能无法提供足够的信息；agent还必须尝试任务来推断出一个策略。在本项工作中，我们提出了一个可以学习从demo和稀疏奖励反馈的试验经验中学习的方法。和元模仿学习相比，这个方法可以使agent有效且高效地自动改进自己在demo数据之外。和元强化学习相比,我们可以扩展到更广泛的任务分配，因为demo减少了探索的负担。我们的实验表明，我们的方法在一组具有挑战性的，基于视觉的控制任务上显著优于先前的方法。
- 创新模式 | 老问题首次引入某种新技术或提出新方法或改进
- 技术总结 | 单agent 元强化学习 迁移适应能力
- 关键词 | 模仿强化学习 高效探索采样
- 源代码 | https://github.com/google-research/tensor2robot/tree/master/research/vrgripper
- 贡献 | 提出了一个可以用一个demo和试错经验学习新行为的元学习算法，在接受了一个新目标的演示后，经过元训练的agent可以学习通过少量的尝试和错误来完成这个目标，而只有二进制的“成功或失败”标签。我们对一个具有挑战性的、基于视觉的控制问题评估了我们的算法和几个先前的方法，这个控制问题涉及四个不同类型任务的操作任务:按按钮、抓取、推按钮和取放按钮。我们发现，通过一次演示和一次试验，我们的方法可以有效地学习新的、外置的对象，同时显著优于元模仿学习、元强化学习和先前的演示和奖励反馈相结合的方法。据我们所知，我们的实验是第一个证明元学习能够使一个主体通过原始像素观察的二元强化信号适应新的任务，我们用一个单一的元模型来展示各种不同的操作任务。

**84.Population-Guided Parallel Policy Search for Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rJeINp4KwH
- 作者 | Whiyoung Jung, Giseung Park, Youngchul Sung
- 负责 | 韩帅

**85.Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=HJgcvJBFvB
- 作者 | Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee
- 单位 | University of Michigan; Google Brain
- 负责 | 穆亭江

**86.On the Weaknesses of Reinforcement Learning for Neural Machine Translation**
- 链接 | https://openreview.net/pdf?id=H1eCw3EKvH
- 作者 | Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend
- 负责 | 王翔森

**87.State Alignment-based Imitation Learning**
- 链接 | https://openreview.net/pdf?id=rylrdxHFDr
- 作者 | Fangchen Liu, Zhan Ling, Tongzhou Mu, Hao Su
- 单位 | University of California San Diego
- 负责 | 郭智慧
- 摘要 | 考虑一个模仿学习问题，模仿者和专家有不同的动力学模型。目前大多数模仿学习方法都失败了，因为它们专注于模仿动作。我们提出了一种新的基于状态对齐的模仿学习方法，以训练模仿者尽可能地跟随专家演示中的状态序列。状态对齐来自局部和全局两个角度，我们通过一个规则化的政策更新目标将它们结合到一个强化学习框架中。在标准模仿学习设置和专家与模仿者具有不同动力学模型的模仿学习设置上，证明了该方法的优越性。
- 创新模式 | 将某个已经使用了RL方法的应用或理论问题进行问题场景细分（针对细分场景的问题开展研究）
- 技术总结 | 单agent 迁移适应能力
- 关键词 | 鲁棒性 模仿强化学习 算法理论研究 迁移强化学习 环境变化策略适应
- 源代码 | https://github.com/tgangwani/RL-Indirect-imitation
- 贡献 | 1、提出了一种基于状态对齐的方法用于专家和模仿者动力学不一致的仿真学习问题。
          2、提出了一种基于β-VAE的局部状态对准方法和一种基于Wasserstein距离的全局状态对准方法。
          3、通过一个规则化的策略更新目标，将局部对齐和全局对齐组件结合成一个强化学习框架。

**88.Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents**
- 链接 | https://openreview.net/pdf?id=rylvYaNYDH
- 作者 | Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal
- 单位 | University of Oxford; Element AI; MILA
- 负责 | 宋明惠

**89.Model-Augmented Actor-Critic: Backpropagating through Paths**
- 链接 | https://openreview.net/pdf?id=Skln2A4YDB
- 作者 | Ignasi Clavera, Yao Fu, Pieter Abbeel
- 负责 | 沈硕

**90.Behaviour Suite for Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rygf-kSYwH
- 作者 | Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt
- 单位 | DeepMind
- 负责 | 刘辰宇

**91.Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=BJluxREKDB
- 作者 | Gil Lederman, Markus Rabe, Sanjit Seshia, Edward A. Lee
- 单位 | UC Berkeley; Google Research
- 负责 | 林晨

**92.Maxmin Q-learning: Controlling the Estimation Bias of Q-learning**
- 链接 | https://openreview.net/pdf?id=Bkg0u3Etwr
- 作者 | Qingfeng Lan, Yangchen Pan, Alona Fyshe, Martha White
- 单位 | University of Alberta
- 负责 | 赵亮

**93.Hypermodels for Exploration**
- 链接 | https://openreview.net/pdf?id=ryx6WgStPB
- 作者 | Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, Benjamin Van Roy

**94.Sub-policy Adaptation for Hierarchical Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=ByeWogStDS
- 作者 | Alexander Li, Carlos Florensa, Ignasi Clavera, Pieter Abbeel
- 单位 | UC Berkeley
- 负责 | 穆亭江

**95.SVQN: Sequential Variational Soft Q-Learning Networks**
- 链接 | https://openreview.net/pdf?id=r1xPh2VtPB
- 作者 | Shiyu Huang, Hang Su, Jun Zhu, Ting Chen
- 单位 | Tsinghua University
- 负责 | 王翔森

**96.IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks**
- 链接 | https://openreview.net/pdf?id=BJeGlJStPr
- 作者 | Michael Luo, Jiahao Yao, Richard Liaw, Eric Liang, Ion Stoica
- 单位 | UC Berkeley
- 负责 | 蒋檄文

**97.Ranking Policy Gradient**
- 链接 | https://openreview.net/pdf?id=rJld3hEYvS
- 作者 | Kaixiang Lin, Jiayu Zhou
- 单位 | Michigan State University
- 负责 | 刘辰宇

**98.Model-based reinforcement learning for biological sequence design**
- 链接 | https://openreview.net/pdf?id=HklxbgBKvr
- 作者 | Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, Lucy Colwell
- 单位 | Google Research; Caltech
- 负责 | 胡小波

**99.Learning Nearly Decomposable Value Functions Via Communication Minimization**
- 链接 | https://openreview.net/pdf?id=HJx-3grYDB
- 作者 | Tonghan Wang, Jianhao Wang, Chongyi Zheng, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 王翔森

**100.Implementing Inductive bias for different navigation tasks through diverse RNN attrractors**
- 链接 | https://openreview.net/pdf?id=Byx4NkrtDS
- 作者 | Tie XU, Omri Barak
- 负责 | 宋明惠

**101.Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control**
- 链接 | https://openreview.net/pdf?id=SylL0krYPS
- 作者 | Tsui-Wei Weng, Krishnamurthy (Dj) Dvijotham, Jonathan Uesato, Kai Xiao, Sven Gowal, Robert Stanforth, Pushmeet Kohli
- 单位 | MIT; DeepMind
- 负责 | 林晨

**102.Learning Efficient Parameter Server Synchronization Policies for Distributed SGD**
- 链接 | https://openreview.net/pdf?id=rJxX8T4Kvr
- 作者 | Rong Zhu, Sheng Yang, Andreas Pfadler, Zhengping Qian, Jingren Zhou

**103.Episodic Reinforcement Learning with Associative Memory**
- 链接 | https://openreview.net/pdf?id=HkxjqxBYDB
- 作者 | Guangxiang Zhu, Zichuan Lin, Guangwen Yang, Chongjie Zhang
- 单位 | Tsinghua University
- 负责 | 王翔森

**104.Logic and the 2-Simplicial Transformer**
- 链接 | https://openreview.net/pdf?id=rkecJ6VFvr
- 作者 | James Clift, Dmitry Doryn, Daniel Murfet, James Wallbridge
- 单位 | University of Melbourne

**105.Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning**
- 链接 | https://openreview.net/pdf?id=rkl3m1BFDB
- 作者 | Akanksha Atrey, Kaleigh Clary, David Jensen
- 单位 | University of Massachusetts Amherst
- 负责 | 王翔森

**106.Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP**
- 链接 | https://openreview.net/pdf?id=S1xnXRVFwH
- 作者 | Haonan Yu, Sergey Edunov, Yuandong Tian, Ari S. Morcos
- 单位 | Facebook AI Research
- 负责 | 林晨
